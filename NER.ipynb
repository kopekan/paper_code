{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate3 as evaluate\n",
    "import pandas as pd\n",
    "import preprocessing as preprocess\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense,Dropout,Embedding,LSTM,Bidirectional, Masking, TimeDistributed, Conv1D, MaxPooling1D, Flatten, concatenate, GRU\n",
    "# from tensorflow_addons.layers.crf import CRF\n",
    "from crf import CRF\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "idx2word, word2idx,char_embeddings = preprocess.get_pretrain_char_emb('CKIP')\n",
    "char_vocabs = list(idx2word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus)!=0:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs visible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data (as BERT format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msra bert\n",
      "people bert\n",
      "weibo bert\n",
      "singer bert\n",
      "homeapp bert\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    import time\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from random import shuffle\n",
    "    msra_dest = ['./data/MSRA/msra_train_bioes.txt','./data/MSRA/msra_test_bioes.txt']\n",
    "    msra_zhNERTF = ['./data/MSRA/train_data_zh-NER-TF_bioes.txt','./data/MSRA/train_data_zh-NER-TF_bioes.txt']\n",
    "    peopledaily_dest = ['./data/PeopleDaily/example_bioes.train','./data/PeopleDaily/example_bioes.test']\n",
    "    weibo_dest = ['./data/Weibo/weiboNER_2nd_conll_bioes.train', './data/Weibo/weiboNER_2nd_conll_bioes.test']\n",
    "    homeapp_dest = ['./data/homeapp/家電NER.txt', './data/homeapp/家電NER.txt']\n",
    "    singer_dest=['./data/Singer/SingerData.txt', './data/Singer/SingerData.txt']\n",
    "    print('msra bert')\n",
    "    (msra_train_x_bert, msra_train_y_bert, _), (msra_test_x_bert, msra_test_y_bert, _),  msra_tags = preprocess.load_data(msra_dest[0], msra_dest[1], True)\n",
    "    print('people bert')\n",
    "    (people_train_x_bert, people_train_y_bert, _), (people_test_x_bert, people_test_y_bert, _), people_tags = preprocess.load_data(peopledaily_dest[0], peopledaily_dest[1], True)\n",
    "    print('weibo bert')\n",
    "    (weibo_train_x_bert, weibo_train_y_bert, _), (weibo_test_x_bert, weibo_test_y_bert, _),  weibo_tags = preprocess.load_data(weibo_dest[0], weibo_dest[1], True)\n",
    "    print('singer bert')\n",
    "    (singer_train_x_bert, singer_train_y_bert, _), (_, _, _),  singer_tags = preprocess.load_data(singer_dest[0], singer_dest[1], True)\n",
    "    print('homeapp bert')\n",
    "    (homeapp_train_x_bert, homeapp_train_y_bert, _), (_, _, _),  homeapp_tags = preprocess.load_data(homeapp_dest[0], homeapp_dest[1], use_bert=True)\n",
    "    msra_label2idx = {char: idx for idx, char in enumerate(msra_tags)}\n",
    "    msra_idx2label = {idx: label for label, idx in msra_label2idx.items()}\n",
    "    people_label2idx = {char: idx for idx, char in enumerate(people_tags)}\n",
    "    people_idx2label = {idx: label for label, idx in people_label2idx.items()}\n",
    "    weibo_label2idx = {char: idx for idx, char in enumerate(weibo_tags)}\n",
    "    weibo_idx2label = {idx: label for label, idx in weibo_label2idx.items()}\n",
    "    singer_label2idx = {char: idx for idx, char in enumerate(singer_tags)}\n",
    "    singer_idx2label = {idx: label for label, idx in singer_label2idx.items()}\n",
    "    homeapp_tags.append('S-PB')\n",
    "    homeapp_tags=list(set(homeapp_tags))\n",
    "    homeapp_tags.sort()\n",
    "    homeapp_label2idx = {char: idx for idx, char in enumerate(homeapp_tags)}\n",
    "    homeapp_idx2label = {idx: label for label, idx in homeapp_label2idx.items()}\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear ow and at\n",
    "for i in range(len(homeapp_train_y_bert)):\n",
    "    for j in range(len(homeapp_train_y_bert[i])):\n",
    "        if len(homeapp_idx2label[homeapp_train_y_bert[i][j]])>1:\n",
    "            if homeapp_idx2label[homeapp_train_y_bert[i][j]][-2:]=='OW' or homeapp_idx2label[homeapp_train_y_bert[i][j]][-2:]=='AT':\n",
    "                homeapp_train_y_bert[i][j]=homeapp_label2idx['O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mytag, domain_num=2, crf_layer=True, compile_ = True, use_bert=False, embed_size = 50, pretrain_emb=False): #compile first\n",
    "    if use_bert:\n",
    "        input1 = Input(shape=(128,), name='input_word_ids', dtype=tf.int32)\n",
    "        input2 = Input(shape=(128,), name='input_mask', dtype=tf.int32)\n",
    "        input3 = Input(shape=(128,), name='input_type_ids', dtype=tf.int32)\n",
    "        #no fintune\n",
    "#        bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3', trainable=False, output_key='sequence_output', name='bert')\n",
    "        #fintune\n",
    "        bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3', trainable=True, output_key='sequence_output', name='encoding_bert')\n",
    "        output = bert_layer({'input_word_ids':input1, 'input_mask':input2, 'input_type_ids':input3})\n",
    "    else:\n",
    "        inputs = Input(shape=(128,), name='encoding_input')\n",
    "        output = Masking(mask_value=word2idx['<PAD>'], name='encoding_mask')(inputs)\n",
    "        if pretrain_emb:\n",
    "            embed_size = len(char_embeddings['我'])\n",
    "            embedding_matrix = np.zeros((len(word2idx), embed_size))\n",
    "            for word, i in word2idx.items():\n",
    "                embedding_vector = char_embeddings.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "            output = Embedding(len(char_vocabs), embed_size, weights=[embedding_matrix], trainable=True, name='encoding_emb')(output)  \n",
    "        else:\n",
    "            output = Embedding(len(char_vocabs), embed_size, trainable=True, name='encoding_emb')(output)  \n",
    "        \n",
    "    encoding_output = Bidirectional(LSTM(200 // 2, return_sequences=True, trainable=True), name='encoding_lstm')(output)\n",
    "        \n",
    "    decoding_output = TimeDistributed(Dense(len(mytag)), name='decoding_timedistribute')(encoding_output)\n",
    "    if crf_layer:\n",
    "        crf=CRF(len(mytag),name='decoding_crf_layer')\n",
    "        decoding_output = crf(decoding_output)\n",
    "    else:\n",
    "        decoding_output = Dense(len(mytag), activation='softmax', name='decoding_softmax')(decoding_output)\n",
    "    if use_bert:\n",
    "        decoding_model = Model(inputs = [input1, input2, input3], outputs = decoding_output)\n",
    "    else:    \n",
    "        decoding_model = Model(inputs = inputs, outputs = decoding_output)\n",
    "    \n",
    "    temp_output = GradReverse()(encoding_output)\n",
    "    cnn = Conv1D(1, kernel_size=1, strides=1,activation='relu', name='dis_conv')(temp_output)\n",
    "    cnn = MaxPooling1D(10, name='dis_maxpooling1d')(cnn)\n",
    "    flat = Flatten(name='dis_flatten')(cnn)\n",
    "    discriminator_output = Dense(domain_num, activation='softmax', name = 'discriminator')(flat) #softmax會讓所有的output總和=1\n",
    "    if use_bert:\n",
    "        discriminator_model = Model(inputs = [input1, input2, input3], outputs = discriminator_output)\n",
    "    else:\n",
    "        discriminator_model = Model(inputs = inputs, outputs = discriminator_output)\n",
    "    \n",
    "    if compile_:\n",
    "        if crf_layer:\n",
    "            if use_bert:\n",
    "                import model.optimization as optimization\n",
    "                optimizer = optimization.create_optimizer(5e-5, (1280//32)*epochs, int((epochs*1280*0.1)//32), 0.0, 'adamw')\n",
    "                decoding_model.compile(optimizer=optimizer,loss = crf.get_loss, metrics=[crf.get_accuracy])\n",
    "            else:\n",
    "                decoding_model.compile(optimizer=\"adam\",loss = crf.get_loss, metrics=[crf.get_accuracy])\n",
    "        else:\n",
    "            decoding_model.compile(optimizer=\"adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        discriminator_model.compile(optimizer=\"adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    output_model = {'decoding_model':decoding_model, 'discriminator_model':discriminator_model}    \n",
    "    \n",
    "    print('【create model】')\n",
    "    if not use_bert:        \n",
    "        print('embedding size:', embed_size)\n",
    "        print('use pretrain embedding:', pretrain_emb)\n",
    "    print('domain number:', domain_num)\n",
    "    print('tag size:', len(mytag))\n",
    "    print('use BERT?', use_bert)\n",
    "\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#read other pre-trained model\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3', trainable=True, output_key='sequence_output', name='encoding_bert')\n",
    "from tensorflow.keras.models import load_model\n",
    "print('loading model...')\n",
    "# decoding_final_model = load_model('./MetaNER_weight_save/bert531_homeapp_sgd.h5', custom_objects={'KerasLayer':bert_layer})\n",
    "decoding_final_model = load_model('./MetaNER_weight_save/bert531_test_on_singer.h5', custom_objects={'KerasLayer':bert_layer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = list(np.load('LC_test1000samples_idx.npy'))\n",
    "test_bert = {'X':{key: np.array([homeapp_train_x_bert[key][i] for i in test_samples]) for key in homeapp_train_x_bert.keys()} , 'y':np.array([homeapp_train_y_bert[i] for i in test_samples])}\n",
    "train_samples = [i for i in range(2000) if i not in test_samples]\n",
    "train_bert = {'X':{key: np.array([homeapp_train_x_bert[key][i] for i in train_samples]) for key in homeapp_train_x_bert.keys()}, 'y':np.array([homeapp_train_y_bert[i] for i in train_samples])} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use meta? True\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 100\n",
      "meta:True, train size:100, test size:1000\n",
      "Epoch 1/8\n",
      "7/7 [==============================] - 20s 400ms/step - loss: 437.2680 - get_accuracy: 0.0118\n",
      "Epoch 2/8\n",
      "7/7 [==============================] - 3s 391ms/step - loss: 233.3502 - get_accuracy: 0.5014\n",
      "Epoch 3/8\n",
      "7/7 [==============================] - 3s 397ms/step - loss: 61.8656 - get_accuracy: 0.9443\n",
      "Epoch 4/8\n",
      "7/7 [==============================] - 3s 391ms/step - loss: 45.9870 - get_accuracy: 0.9437\n",
      "Epoch 5/8\n",
      "7/7 [==============================] - 3s 390ms/step - loss: 39.8132 - get_accuracy: 0.9401\n",
      "Epoch 6/8\n",
      "7/7 [==============================] - 3s 388ms/step - loss: 28.5321 - get_accuracy: 0.9410\n",
      "Epoch 7/8\n",
      "7/7 [==============================] - 3s 387ms/step - loss: 19.7306 - get_accuracy: 0.9540\n",
      "Epoch 8/8\n",
      "7/7 [==============================] - 3s 393ms/step - loss: 15.7968 - get_accuracy: 0.9633\n",
      "label type: BIOES\n",
      "Accuracy:  124005 / 128000 = 0.9687890625\n",
      "gold_num =  3194  pred_num =  2962  right_num =  1367\n",
      "f1: 0.4441195581546459\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.4021  0.2062  0.2726\n",
      "PN     0.4866  0.6842  0.5687\n",
      "total  0.4600  0.4300  0.4400\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0   353  1014   1367\n",
      "predict   0   0   878  2084   2962\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 57 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 200\n",
      "meta:True, train size:200, test size:1000\n",
      "Epoch 1/8\n",
      "13/13 [==============================] - 21s 415ms/step - loss: 372.2148 - get_accuracy: 0.1232\n",
      "Epoch 2/8\n",
      "13/13 [==============================] - 5s 406ms/step - loss: 48.4534 - get_accuracy: 0.9427\n",
      "Epoch 3/8\n",
      "13/13 [==============================] - 5s 414ms/step - loss: 25.0706 - get_accuracy: 0.9480\n",
      "Epoch 4/8\n",
      "13/13 [==============================] - 5s 414ms/step - loss: 15.7075 - get_accuracy: 0.9681\n",
      "Epoch 5/8\n",
      "13/13 [==============================] - 5s 406ms/step - loss: 10.1655 - get_accuracy: 0.9844\n",
      "Epoch 6/8\n",
      "13/13 [==============================] - 5s 417ms/step - loss: 6.4859 - get_accuracy: 0.9900\n",
      "Epoch 7/8\n",
      "13/13 [==============================] - 5s 405ms/step - loss: 3.8336 - get_accuracy: 0.9963\n",
      "Epoch 8/8\n",
      "13/13 [==============================] - 5s 413ms/step - loss: 2.8578 - get_accuracy: 0.9972\n",
      "label type: BIOES\n",
      "Accuracy:  126727 / 128000 = 0.9900546875\n",
      "gold_num =  3194  pred_num =  3471  right_num =  2949\n",
      "f1: 0.8849212303075769\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.8450  0.9141  0.8782\n",
      "PN     0.8548  0.9339  0.8926\n",
      "total  0.8500  0.9200  0.8800\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1565  1384   2949\n",
      "predict   0   0  1852  1619   3471\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 77 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 300\n",
      "meta:True, train size:300, test size:1000\n",
      "Epoch 1/8\n",
      "19/19 [==============================] - 24s 419ms/step - loss: 168.2997 - get_accuracy: 0.6359\n",
      "Epoch 2/8\n",
      "19/19 [==============================] - 8s 420ms/step - loss: 31.9580 - get_accuracy: 0.9439\n",
      "Epoch 3/8\n",
      "19/19 [==============================] - 8s 415ms/step - loss: 15.1576 - get_accuracy: 0.9628\n",
      "Epoch 4/8\n",
      "19/19 [==============================] - 8s 416ms/step - loss: 9.3654 - get_accuracy: 0.9819\n",
      "Epoch 5/8\n",
      "19/19 [==============================] - 8s 418ms/step - loss: 4.6871 - get_accuracy: 0.9940\n",
      "Epoch 6/8\n",
      "19/19 [==============================] - 8s 419ms/step - loss: 3.3500 - get_accuracy: 0.9966\n",
      "Epoch 7/8\n",
      "19/19 [==============================] - 8s 417ms/step - loss: 1.9695 - get_accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "19/19 [==============================] - 8s 422ms/step - loss: 1.6010 - get_accuracy: 0.9986\n",
      "label type: BIOES\n",
      "Accuracy:  126982 / 128000 = 0.992046875\n",
      "gold_num =  3194  pred_num =  3282  right_num =  2959\n",
      "f1: 0.913835701050031\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.8944  0.9153  0.9047\n",
      "PN     0.9098  0.9393  0.9243\n",
      "total  0.9000  0.9300  0.9100\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1567  1392   2959\n",
      "predict   0   0  1752  1530   3282\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 101 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 400\n",
      "meta:True, train size:400, test size:1000\n",
      "Epoch 1/8\n",
      "25/25 [==============================] - 23s 358ms/step - loss: 289.9544 - get_accuracy: 0.3190\n",
      "Epoch 2/8\n",
      "25/25 [==============================] - 9s 363ms/step - loss: 31.7389 - get_accuracy: 0.9443\n",
      "Epoch 3/8\n",
      "25/25 [==============================] - 9s 362ms/step - loss: 14.6961 - get_accuracy: 0.9664\n",
      "Epoch 4/8\n",
      "25/25 [==============================] - 9s 359ms/step - loss: 6.1870 - get_accuracy: 0.9904\n",
      "Epoch 5/8\n",
      "25/25 [==============================] - 9s 366ms/step - loss: 3.6972 - get_accuracy: 0.9954\n",
      "Epoch 6/8\n",
      "25/25 [==============================] - 9s 363ms/step - loss: 2.4142 - get_accuracy: 0.9976\n",
      "Epoch 7/8\n",
      "25/25 [==============================] - 9s 361ms/step - loss: 1.8408 - get_accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "25/25 [==============================] - 9s 361ms/step - loss: 1.4537 - get_accuracy: 0.9985\n",
      "label type: BIOES\n",
      "Accuracy:  127100 / 128000 = 0.99296875\n",
      "gold_num =  3194  pred_num =  3310  right_num =  2997\n",
      "f1: 0.9215867158671587\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.8999  0.9352  0.9172\n",
      "PN     0.9118  0.9420  0.9267\n",
      "total  0.9100  0.9400  0.9200\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1601  1396   2997\n",
      "predict   0   0  1779  1531   3310\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 108 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 500\n",
      "meta:True, train size:500, test size:1000\n",
      "Epoch 1/8\n",
      "32/32 [==============================] - 33s 415ms/step - loss: 283.1151 - get_accuracy: 0.4263\n",
      "Epoch 2/8\n",
      "32/32 [==============================] - 13s 421ms/step - loss: 25.1207 - get_accuracy: 0.9477\n",
      "Epoch 3/8\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 13.9277 - get_accuracy: 0.9710\n",
      "Epoch 4/8\n",
      "32/32 [==============================] - 13s 415ms/step - loss: 5.8226 - get_accuracy: 0.9917\n",
      "Epoch 5/8\n",
      "32/32 [==============================] - 13s 415ms/step - loss: 3.4873 - get_accuracy: 0.9964\n",
      "Epoch 6/8\n",
      "32/32 [==============================] - 13s 415ms/step - loss: 2.7750 - get_accuracy: 0.9970\n",
      "Epoch 7/8\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 1.7448 - get_accuracy: 0.9983\n",
      "Epoch 8/8\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 1.4449 - get_accuracy: 0.9987\n",
      "label type: BIOES\n",
      "Accuracy:  127100 / 128000 = 0.99296875\n",
      "gold_num =  3194  pred_num =  3295  right_num =  2994\n",
      "f1: 0.922792417938049\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.9066  0.9357  0.9209\n",
      "PN     0.9110  0.9393  0.9249\n",
      "total  0.9100  0.9400  0.9200\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1602  1392   2994\n",
      "predict   0   0  1767  1528   3295\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 144 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 600\n",
      "meta:True, train size:600, test size:1000\n",
      "Epoch 1/8\n",
      "38/38 [==============================] - 32s 419ms/step - loss: 201.8586 - get_accuracy: 0.5088\n",
      "Epoch 2/8\n",
      "38/38 [==============================] - 16s 423ms/step - loss: 21.2949 - get_accuracy: 0.9568\n",
      "Epoch 3/8\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 9.7087 - get_accuracy: 0.9871\n",
      "Epoch 4/8\n",
      "38/38 [==============================] - 16s 422ms/step - loss: 5.3671 - get_accuracy: 0.9945\n",
      "Epoch 5/8\n",
      "38/38 [==============================] - 16s 421ms/step - loss: 3.3743 - get_accuracy: 0.9967\n",
      "Epoch 6/8\n",
      "38/38 [==============================] - 16s 417ms/step - loss: 2.6754 - get_accuracy: 0.9969\n",
      "Epoch 7/8\n",
      "38/38 [==============================] - 16s 423ms/step - loss: 2.6433 - get_accuracy: 0.9970\n",
      "Epoch 8/8\n",
      "38/38 [==============================] - 16s 420ms/step - loss: 1.7809 - get_accuracy: 0.9983\n",
      "label type: BIOES\n",
      "Accuracy:  127219 / 128000 = 0.9938984375\n",
      "gold_num =  3194  pred_num =  3293  right_num =  3016\n",
      "f1: 0.9298597194388778\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.9028  0.9381  0.9201\n",
      "PN     0.9313  0.9514  0.9412\n",
      "total  0.9200  0.9400  0.9300\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1606  1410   3016\n",
      "predict   0   0  1779  1514   3293\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 162 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 700\n",
      "meta:True, train size:700, test size:1000\n",
      "Epoch 1/8\n",
      "44/44 [==============================] - 34s 421ms/step - loss: 345.7154 - get_accuracy: 0.3964\n",
      "Epoch 2/8\n",
      "44/44 [==============================] - 19s 422ms/step - loss: 38.4789 - get_accuracy: 0.9435\n",
      "Epoch 3/8\n",
      "44/44 [==============================] - 19s 424ms/step - loss: 18.1415 - get_accuracy: 0.9600\n",
      "Epoch 4/8\n",
      "44/44 [==============================] - 19s 423ms/step - loss: 7.7740 - get_accuracy: 0.9908\n",
      "Epoch 5/8\n",
      "44/44 [==============================] - 19s 423ms/step - loss: 4.0349 - get_accuracy: 0.9963\n",
      "Epoch 6/8\n",
      "44/44 [==============================] - 19s 426ms/step - loss: 3.1185 - get_accuracy: 0.9966\n",
      "Epoch 7/8\n",
      "44/44 [==============================] - 19s 423ms/step - loss: 2.5102 - get_accuracy: 0.9974\n",
      "Epoch 8/8\n",
      "44/44 [==============================] - 19s 426ms/step - loss: 2.1016 - get_accuracy: 0.9980\n",
      "label type: BIOES\n",
      "Accuracy:  127169 / 128000 = 0.9935078125\n",
      "gold_num =  3194  pred_num =  3326  right_num =  3010\n",
      "f1: 0.9233128834355828\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.8978  0.9340  0.9155\n",
      "PN     0.9133  0.9521  0.9323\n",
      "total  0.9000  0.9400  0.9200\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1599  1411   3010\n",
      "predict   0   0  1781  1545   3326\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 188 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 800\n",
      "meta:True, train size:800, test size:1000\n",
      "Epoch 1/8\n",
      "50/50 [==============================] - 32s 365ms/step - loss: 137.5304 - get_accuracy: 0.6434\n",
      "Epoch 2/8\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 11.8679 - get_accuracy: 0.9780\n",
      "Epoch 3/8\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 4.3593 - get_accuracy: 0.9939\n",
      "Epoch 4/8\n",
      "50/50 [==============================] - 18s 366ms/step - loss: 2.4496 - get_accuracy: 0.9965\n",
      "Epoch 5/8\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 1.8464 - get_accuracy: 0.9977\n",
      "Epoch 6/8\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 1.4328 - get_accuracy: 0.9981\n",
      "Epoch 7/8\n",
      "50/50 [==============================] - 18s 365ms/step - loss: 1.4363 - get_accuracy: 0.9983\n",
      "Epoch 8/8\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 1.0518 - get_accuracy: 0.9986\n",
      "label type: BIOES\n",
      "Accuracy:  127214 / 128000 = 0.993859375\n",
      "gold_num =  3194  pred_num =  3256  right_num =  3006\n",
      "f1: 0.932093023255814\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.9223  0.9357  0.9290\n",
      "PN     0.9243  0.9474  0.9357\n",
      "total  0.9200  0.9400  0.9300\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1602  1404   3006\n",
      "predict   0   0  1737  1519   3256\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 181 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 900\n",
      "meta:True, train size:900, test size:1000\n",
      "Epoch 1/8\n",
      "57/57 [==============================] - 40s 421ms/step - loss: 230.3195 - get_accuracy: 0.5201\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 24s 424ms/step - loss: 10.6336 - get_accuracy: 0.9791\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 24s 425ms/step - loss: 4.4480 - get_accuracy: 0.9940\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 24s 422ms/step - loss: 2.8237 - get_accuracy: 0.9966\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 24s 424ms/step - loss: 2.3724 - get_accuracy: 0.9971\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 24s 424ms/step - loss: 1.8004 - get_accuracy: 0.9981\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 24s 421ms/step - loss: 1.6481 - get_accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 24s 423ms/step - loss: 1.8349 - get_accuracy: 0.9980\n",
      "label type: BIOES\n",
      "Accuracy:  127264 / 128000 = 0.99425\n",
      "gold_num =  3194  pred_num =  3267  right_num =  3013\n",
      "f1: 0.9326729608419749\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.9217  0.9346  0.9281\n",
      "PN     0.9229  0.9534  0.9379\n",
      "total  0.9200  0.9400  0.9300\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1600  1413   3013\n",
      "predict   0   0  1736  1531   3267\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 230 sec.\n",
      "【create model】\n",
      "domain number: 2\n",
      "tag size: 17\n",
      "use BERT? True\n",
      "training data: 1000\n",
      "meta:True, train size:1000, test size:1000\n",
      "Epoch 1/8\n",
      "63/63 [==============================] - 46s 422ms/step - loss: 234.0674 - get_accuracy: 0.5634\n",
      "Epoch 2/8\n",
      "63/63 [==============================] - 27s 425ms/step - loss: 13.7279 - get_accuracy: 0.9701\n",
      "Epoch 3/8\n",
      "63/63 [==============================] - 27s 422ms/step - loss: 4.4708 - get_accuracy: 0.9940\n",
      "Epoch 4/8\n",
      "63/63 [==============================] - 27s 426ms/step - loss: 2.6337 - get_accuracy: 0.9961\n",
      "Epoch 5/8\n",
      "63/63 [==============================] - 27s 424ms/step - loss: 1.9018 - get_accuracy: 0.9973\n",
      "Epoch 6/8\n",
      "63/63 [==============================] - 27s 423ms/step - loss: 1.6295 - get_accuracy: 0.9978\n",
      "Epoch 7/8\n",
      "63/63 [==============================] - 27s 423ms/step - loss: 1.6491 - get_accuracy: 0.9977\n",
      "Epoch 8/8\n",
      "63/63 [==============================] - 27s 425ms/step - loss: 1.6097 - get_accuracy: 0.9977\n",
      "label type: BIOES\n",
      "Accuracy:  127208 / 128000 = 0.9938125\n",
      "gold_num =  3194  pred_num =  3301  right_num =  3020\n",
      "f1: 0.9299461123941493\n",
      "            P       R       F\n",
      "AT     0.0000  0.0000  0.0000\n",
      "OW     0.0000  0.0000  0.0000\n",
      "PB     0.9100  0.9387  0.9241\n",
      "PN     0.9205  0.9534  0.9367\n",
      "total  0.9100  0.9500  0.9300\n",
      "result count:\n",
      "         AT  OW    PB    PN  total\n",
      "true      0   0  1607  1413   3020\n",
      "predict   0   0  1766  1535   3301\n",
      "answer    0   0  1712  1482   3194\n",
      "spend 251 sec.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#with tf.device('/cpu:0'):\n",
    "#with meta\n",
    "from tensorflow.keras import callbacks\n",
    "import time\n",
    "meta = True\n",
    "print('use meta?', meta)\n",
    "epochs = 8\n",
    "all_f1_table = 0\n",
    "data_sizes = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "# data_sizes = [1000]\n",
    "for i in data_sizes:\n",
    "    starttime = time.time()\n",
    "    model = create_model(homeapp_tags, use_bert=True)['decoding_model']\n",
    "    x, y = sample_data([train_bert['X'], train_bert['y']], random_=False, datasize=i)\n",
    "    print('training data:', len(y))\n",
    "    if meta:        \n",
    "        temp_weight = model_get_weight(temp_model[0], keyword='enc')\n",
    "        update_weights(model, temp_weight, keyword='enc')\n",
    "    print('meta:{}, train size:{}, test size:{}'.format(meta, len(y), len(test_bert['y'])))\n",
    "#     history = model.fit(train_bert['X'],train_bert['y'],batch_size=16, epochs=epochs, verbose=1)\n",
    "    history = model.fit(x,y,batch_size=16, epochs=epochs, verbose=1)\n",
    "    pred = [[j for j in i] for i in model.predict(test_bert['X'])]   \n",
    "\n",
    "    _, _, f1_test, f1_table, result_count = evaluate.evaluation(pred, test_bert['y'], True,homeapp_idx2label, homeapp_idx2label)\n",
    "    print('f1:', f1_test)\n",
    "    print(f1_table)\n",
    "    print('result count:')\n",
    "    print(pd.DataFrame(result_count))\n",
    "    print('spend', int(time.time()-starttime),'sec.')\n",
    "    del model\n",
    "#     if type(all_f1_table)==int:\n",
    "#         all_f1_table=f1_table.copy()\n",
    "#     else:\n",
    "#         all_f1_table+=f1_table\n",
    "print('--------------------------------------------------')        \n",
    "# print(all_f1_table/itrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本是想要試在NER上用不同的transfer learning方法\n",
    "\n",
    "但發現幾乎都沒用，所以放棄 \n",
    "\n",
    "下面的程式碼的效能一樣沒有放在論文中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reverse Layer\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradReverse(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new for share encoder\n",
    "def create_src_model(mytag, crf_layer=True, compile_ = True, use_bert=False, src_num = 2, embed_size=50, pretrain_emb=False): #compile first\n",
    "    if type(mytag[0])!=list:\n",
    "        mytag = [mytag]*src_num\n",
    "    elif len(mytag)!=src_num:\n",
    "        src_num=len(mytag)\n",
    "    if use_bert:\n",
    "        input1 = Input(shape=(128,), name='input_word_ids', dtype=tf.int32)\n",
    "        input2 = Input(shape=(128,), name='input_mask', dtype=tf.int32)\n",
    "        input3 = Input(shape=(128,), name='input_type_ids', dtype=tf.int32)\n",
    "        #fintune\n",
    "        bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3', trainable=True, name='encoding_bert')\n",
    "        output = bert_layer({'input_word_ids':input1, 'input_mask':input2, 'input_type_ids':input3})['sequence_output']\n",
    "    else:\n",
    "        inputs = Input(shape=(128,), name='encoding_input')\n",
    "        output = Masking(mask_value=word2idx['<PAD>'], name='encoding_mask')(inputs)\n",
    "        if pretrain_emb:\n",
    "            embed_size = len(char_embeddings['我'])\n",
    "            embedding_matrix = np.zeros((len(word2idx), embed_size))\n",
    "            for word, i in word2idx.items():\n",
    "                embedding_vector = char_embeddings.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "            output = Embedding(len(char_vocabs), embed_size, weights=[embedding_matrix], trainable=True, name='encoding_emb')(output)  \n",
    "        else:\n",
    "            output = Embedding(len(char_vocabs), embed_size, trainable=True, name='encoding_emb')(output)  \n",
    "        \n",
    "    encoding_output = Bidirectional(LSTM(200 // 2, return_sequences=True, trainable=True), name='encoding_lstm')(output)\n",
    "#    encoding_model = Model(inputs = inputs, outputs = encoding_output)\n",
    "\n",
    "    #decoder\n",
    "    decoding_models = []\n",
    "    for i in range(src_num):\n",
    "        decoding_output = TimeDistributed(Dense(len(mytag[i])), name='decoding_timedistribute')(encoding_output)\n",
    "        if crf_layer:\n",
    "            crf=CRF(len(mytag[i]),name='decoding_crf_layer')\n",
    "            decoding_output = crf(decoding_output)\n",
    "        else:\n",
    "            decoding_output = Dense(len(mytag[i]), activation='softmax', name='decoding_softmax')(decoding_output)\n",
    "        if use_bert:\n",
    "            import model.optimization as optimization\n",
    "            optimizer = optimization.create_optimizer(5e-5, (1280//32)*epochs, int((epochs*1280*0.1)//32), 0.0, 'adamw')\n",
    "            decoding_model = Model(inputs = [input1, input2, input3], outputs = decoding_output)\n",
    "            if compile_:\n",
    "                decoding_model.compile(optimizer=optimizer,loss = crf.get_loss, metrics=[crf.get_accuracy])\n",
    "        else:    \n",
    "            decoding_model = Model(inputs = inputs, outputs = decoding_output)\n",
    "        decoding_models.append(decoding_model)\n",
    "    temp_output = bert_layer({'input_word_ids':input1, 'input_mask':input2, 'input_type_ids':input3})['pooled_output']\n",
    "    temp_output = GradReverse()(temp_output)\n",
    "#     cnn = Conv1D(1, kernel_size=1, strides=1,activation='relu', name='dis_conv')(temp_output)\n",
    "#     cnn = MaxPooling1D(10, name='dis_maxpooling1d')(cnn)\n",
    "#     flat = Flatten(name='dis_flatten')(cnn)\n",
    "    discriminator_output = Dense(src_num+1, activation='softmax', name = 'discriminator')(temp_output)\n",
    "    if use_bert:\n",
    "        discriminator_model = Model(inputs = [input1, input2, input3], outputs = discriminator_output)\n",
    "    else:\n",
    "        discriminator_model = Model(inputs = inputs, outputs = discriminator_output)\n",
    "    \n",
    "    if compile_:\n",
    "        discriminator_model.compile(optimizer=\"adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#         if crf_layer:\n",
    "#             if use_bert:\n",
    "#                 import model.optimization as optimization\n",
    "#                 optimizer = optimization.create_optimizer(5e-5, (1280//32)*epochs, int((epochs*1280*0.1)//32), 0.0, 'adamw')\n",
    "#                 for temp in range(src_num):\n",
    "#                     crf=CRF(len(mytag[temp]),name='decoding_crf_layer')\n",
    "#                     decoding_models[temp].compile(optimizer=optimizer,loss = crf.get_loss, metrics=[crf.get_accuracy])\n",
    "#             else:\n",
    "#                 for temp in range(src_num):\n",
    "#                     decoding_models[temp].compile(optimizer='adam',loss = crf.get_loss, metrics=[crf.get_accuracy])\n",
    "#         else:\n",
    "#             for temp in range(src_num):\n",
    "#                decoding_models[temp].compile(optimizer=\"categorical_crossentropy\",loss = crf.get_loss, metrics=[accuracy])\n",
    "    return_model = {'decoding_model':decoding_models, 'discriminator_model':discriminator_model}\n",
    "    print('【create source model】')\n",
    "    if not use_bert:\n",
    "        print('embedding size:', embed_size)\n",
    "        print('use pretrain embedding:', pretrain_emb)\n",
    "    print('domain number:', src_num)\n",
    "    print('tag size:', len(mytag))\n",
    "    print('use crf?', crf_layer)\n",
    "    print('use BERT?', use_bert)\n",
    "    return return_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_get_weight(model, keyword='', not_=False):\n",
    "    origin_weight = []\n",
    "    for layer in model.layers:\n",
    "        if not_:\n",
    "            if not layer.name.startswith(keyword): \n",
    "                origin_weight.append(np.array(layer.get_weights()))\n",
    "        else:\n",
    "            if layer.name.startswith(keyword): \n",
    "                origin_weight.append(np.array(layer.get_weights()))\n",
    "    return np.array(origin_weight)\n",
    "\n",
    "def update_weights(model, update_weight, keyword='', not_=False):\n",
    "    k=0\n",
    "    for layer in model.layers:\n",
    "        if not_:\n",
    "            if not layer.name.startswith(keyword):\n",
    "                layer.set_weights(update_weight[k])\n",
    "                k+=1\n",
    "        else:\n",
    "            if layer.name.startswith(keyword):\n",
    "                layer.set_weights(update_weight[k])\n",
    "                k+=1\n",
    "def update_weights_forsame(model, model_src):\n",
    "    for layer in model.layers:\n",
    "        flag = False\n",
    "        for layer_src in model_src.layers:\n",
    "            if layer.name==layer_src.name and len(layer.get_weights())==len(layer_src.get_weights()) and flag==False:\n",
    "                try: \n",
    "                    layer.set_weights(layer_src.get_weights())\n",
    "                    flag = True\n",
    "                except:\n",
    "                    print('error!')\n",
    "        if flag==False:\n",
    "            print('model layer: \"', layer.name, '\" not in source model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料設定\n",
    "import random\n",
    "def sample_data(data_list, datasize, random_=True):\n",
    "    #data_list = [BERT_x, label]\n",
    "    if random_:\n",
    "        samples = random.sample(range(len(data_list[1])), datasize)\n",
    "    else:\n",
    "        samples = list(range(datasize))        \n",
    "    bert_x = data_list[0]\n",
    "    bert_x = {k:np.array([bert_x[k][i] for i in samples]) for k in bert_x.keys()}\n",
    "    label = np.array(data_list[1])\n",
    "    label = np.array([label[i] for i in samples])\n",
    "    return bert_x, label\n",
    "\n",
    "def sample_data_dann(data_list, datasize, random_=True, start_=0):\n",
    "    #data_list = [[BERT_x, sentiment], [BERT_x, sentiment], [BERT_x, sentiment], ...]\n",
    "    #different domain\n",
    "    bert_x, bert_y, domains = [], [], []\n",
    "    bert_x_wo_tgt, bert_y_wo_tgt = [], []\n",
    "    count=0\n",
    "    for data in data_list:\n",
    "        if random_ and (count!=len(data_list)-1 and start_!=-1):\n",
    "            samples = random.sample(range(len(data[1])), datasize)\n",
    "        else:\n",
    "            end = start_+datasize\n",
    "            if end>len(data[1]):\n",
    "                end = len(data[1])\n",
    "            samples = list(range(start_, end))  \n",
    "        tmp_data_x = [{k:data[0][k][i] for k in data[0].keys()}  for i in samples] #list[dict]\n",
    "        tmp_data_y = [data[1][i] for i in samples]\n",
    "        bert_x+=tmp_data_x\n",
    "        bert_y+=tmp_data_y\n",
    "        if count<len(data_list)-1:\n",
    "            bert_x_wo_tgt.append(transBERTtype(tmp_data_x, True))\n",
    "            bert_y_wo_tgt.append(np.array(tmp_data_y))\n",
    "        domains+=len(samples)*[count]\n",
    "        count+=1\n",
    "    bert_x = transBERTtype(bert_x, True)\n",
    "    label = np.array(bert_y)\n",
    "    domains = np.array(to_categorical(domains))\n",
    "    return bert_x, label, domains, bert_x_wo_tgt, bert_y_wo_tgt\n",
    "\n",
    "def sample_task(train, test=None, support_size=50, query_size=50, domain_num=0):    \n",
    "    #train = [X, y]\n",
    "    #test = [X, y]\n",
    "    #這邊BERT抽出來會變只有一份資料\n",
    "    bert=False\n",
    "    if type(train[0])==dict:\n",
    "        bert=True\n",
    "        train[0] = transBERTtype(train[0], False)\n",
    "        if test!=None:\n",
    "            test[0] = transBERTtype(test[0], False)\n",
    "    train[0] = np.array(train[0])\n",
    "    train[1] = np.array(train[1])\n",
    "    import random\n",
    "#    random.seed()\n",
    "    if test!=None:\n",
    "        test[0] = np.array(test[0])\n",
    "        test[1] = np.array(test[1])\n",
    "        train_samples = random.sample(range(len(train[0])), support_size)\n",
    "        return_train = [train[0][train_samples], train[1][train_samples], [domain_num]*support_size]\n",
    "        other_list = [i for i in range(len(train[0]))] \n",
    "        test_samples = random.sample(range(len(test[0])), query_size)\n",
    "        return_test = [test[0][test_samples], test[1][test_samples], [domain_num]*query_size]\n",
    "    else:\n",
    "        train_samples = random.sample(range(len(train[0])), support_size)\n",
    "        return_train = [train[0][train_samples], train[1][train_samples], [domain_num]*support_size]\n",
    "        other_list = [i for i in range(len(train[0])) if i not in train_samples ] \n",
    "        test_samples = random.sample(other_list, query_size)\n",
    "        return_test = [train[0][test_samples], train[1][test_samples], [domain_num]*query_size]\n",
    "    if bert:\n",
    "        return_train[0] = transBERTtype(return_train[0], True)\n",
    "        return_test[0] = transBERTtype(return_test[0], True)\n",
    "    return return_train, return_test\n",
    "\n",
    "def multiply_grads(lamb, grads):\n",
    "    for i in range(len(grads)):\n",
    "        grads[i]*=lamb\n",
    "    return grads\n",
    "\n",
    "def add_grads(grad1, grad2):\n",
    "    grad = grad1.copy()\n",
    "    try:\n",
    "        for i in range(len(grad)):\n",
    "            grad[i]=grad1[i]+grad2[i]\n",
    "    except:\n",
    "        print('error!')\n",
    "    return grad\n",
    "\n",
    "def filter_sentence(my_x, my_y, myidx2label=None, filt=False, datasize=1500):\n",
    "    bert=False\n",
    "    if type(my_x)==dict:\n",
    "        my_x = transBERTtype(my_x, False)\n",
    "        bert=True\n",
    "    if filt:\n",
    "        outputy = []\n",
    "        if myidx2label!=None:\n",
    "            for ys in range(len(my_y)):\n",
    "                flag = False\n",
    "                for y in my_y[ys]:\n",
    "                    if myidx2label[y]!='O':\n",
    "                        flag=True\n",
    "                        break\n",
    "                if flag:\n",
    "                    outputy.append(ys)\n",
    "        else:\n",
    "            outputy = my_y\n",
    "        if bert:            \n",
    "            return [transBERTtype([my_x[i] for i in outputy[:datasize]], True), [my_y[i] for i in outputy[:datasize]]]\n",
    "        else:\n",
    "            return [[my_x[i] for i in outputy[:datasize]], [my_y[i] for i in outputy[:datasize]]]\n",
    "    else:\n",
    "        if bert:\n",
    "            return [transBERTtype(my_x, True), my_y]\n",
    "        else:\n",
    "            return [my_x, my_y]\n",
    "    \n",
    "def transBERTtype(data, toBERT=True):\n",
    "    if toBERT: #input 每個資料都有三個key，每個key的維度都是128*768\n",
    "        return {k:np.array([data[i][k] for i in range(len(data))]) for k in data[0].keys()}\n",
    "    else: #原本BERT的形式\n",
    "        return [{k:data[k][i] for k in data.keys()} for i in range(len(data['input_word_ids']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2813/2813 [==============================] - 2118s 753ms/step - loss: 7.0682 - get_accuracy: 0.9859\n",
      "1304/1304 [==============================] - 875s 662ms/step - loss: 17.3604 - get_accuracy: 0.9591\n",
      "85/85 [==============================] - 75s 742ms/step - loss: 67.6702 - get_accuracy: 0.9027\n",
      "312/312 [==============================] - 246s 750ms/step - loss: 30.5228 - get_accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "#train on batch\n",
    "#DANN training 的時候用的target x 是train_x; 模型訓練完後還是要像meta learning一樣再去對模型finetune\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "filt_sentence = False\n",
    "\n",
    "my_tags = [msra_tags, people_tags, weibo_tags, singer_tags]\n",
    "# with tf.device('/cpu:0'):\n",
    "#     data_list = [filter_sentence(msra_train_x_bert, msra_train_y_bert, msra_idx2label,filt_sentence), \n",
    "#                 filter_sentence(people_train_x_bert, people_train_y_bert, people_idx2label,filt_sentence), \n",
    "#                 filter_sentence(weibo_train_x_bert, weibo_train_y_bert, weibo_idx2label, filt_sentence), \n",
    "#                 filter_sentence(singer_train_x_bert, singer_train_y_bert, singer_idx2label, filt_sentence)]\n",
    "#     temp_model = create_src_model(my_tags, crf_layer=True, compile_ = True, use_bert=True)['decoding_model']\n",
    "\n",
    "for dta_idx in range(len(data_list)):\n",
    "    history = temp_model[dta_idx].fit(data_list[dta_idx][0],np.array(data_list[dta_idx][1]),batch_size=16, epochs=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reptile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_iteration = 100\n",
    "inner_iteration = 5\n",
    "epochs = inner_iteration\n",
    "datasize_per_task = 16\n",
    "meta_step_size = 0.1\n",
    "import time\n",
    "starttime = time.time()\n",
    "filt_sentence=False\n",
    "my_tags = [msra_tags, people_tags, weibo_tags, singer_tags]\n",
    "source_data = [{'train':filter_sentence(msra_train_x_bert, msra_train_y_bert, msra_idx2label,filt_sentence), 'test':filter_sentence(msra_test_x_bert, msra_test_y_bert, msra_idx2label, filt_sentence), 'tags':msra_tags},\n",
    "               {'train':filter_sentence(people_train_x_bert, people_train_y_bert, people_idx2label,filt_sentence), 'test':filter_sentence(people_test_x_bert, people_test_y_bert, msra_idx2label, filt_sentence), 'tags':people_tags},\n",
    "               {'train':filter_sentence(weibo_train_x_bert, weibo_train_y_bert, weibo_idx2label, filt_sentence), 'test':filter_sentence(weibo_test_x_bert, weibo_test_y_bert, weibo_idx2label, filt_sentence), 'tags':weibo_tags},\n",
    "               {'train':filter_sentence(singer_train_x_bert, singer_train_y_bert, singer_idx2label, filt_sentence), 'test':filter_sentence(singer_train_x_bert, singer_train_y_bert, singer_idx2label, filt_sentence), 'tags':singer_tags}]\n",
    "\n",
    "tmp_model = create_src_model(my_tags, crf_layer=True, compile_ = True, use_bert=True)['decoding_model'] \n",
    "\n",
    "print('start!')\n",
    "# origin_weights = model_get_weight(tmp_model[0], 'enc')\n",
    "# for src_idx in range(len(source_data)):\n",
    "#     update_weights(tmp_model[src_idx], origin_weights, keyword='enc')\n",
    "minloss = 100000000.0\n",
    "flagcount = 0\n",
    "threshold = 5\n",
    "for itr in range(outer_iteration):    \n",
    "    if itr%10==0:\n",
    "        print('itr =',itr)\n",
    "    done_step = itr/outer_iteration\n",
    "    cur_meta_step_size = (1-done_step)*meta_step_size\n",
    "    origin_weights = model_get_weight(tmp_model[0], 'enc')\n",
    "    new_weights = []\n",
    "    losses = []\n",
    "    for srcs in range(len(source_data)): \n",
    "        src_idx = srcs%len(source_data)\n",
    "        train, _ = sample_task(source_data[src_idx]['train'].copy(), domain_num=src_idx, support_size=datasize_per_task)   \n",
    "        for i in range(inner_iteration):\n",
    "            loss = tmp_model[src_idx].train_on_batch(x=train[0], y=train[1])[0]\n",
    "            loss = round(loss, 5)\n",
    "        new_weights.append(model_get_weight(tmp_model[src_idx], keyword='enc'))\n",
    "        update_weights(tmp_model[src_idx], origin_weights, keyword='enc') #這邊的src_idx其實沒差，隨便一個index都可以，因為encoder共用\n",
    "        losses.append(loss)\n",
    "    #update weights\n",
    "    new_weights = np.array(new_weights)\n",
    "    new_weight = new_weights[0]\n",
    "    for i in range(len(new_weights)-1):\n",
    "        new_weight+=new_weights[i+1]\n",
    "    new_weight/=len(new_weights)    \n",
    "    new_weight = origin_weights + ((new_weight-origin_weights)*cur_meta_step_size)\n",
    "    update_weights(tmp_model[0], new_weight, keyword='enc')\n",
    "    print('lr', round(cur_meta_step_size, 5), '\\tloss', losses, '\\t spend', int(time.time()-starttime))\n",
    "    if sum(losses)<minloss:\n",
    "        minloss = sum(losses)\n",
    "        flagcount = 0\n",
    "    else:\n",
    "        flagcount+=1\n",
    "    if flagcount==5:\n",
    "        break\n",
    "    del new_weight, origin_weights, new_weights\n",
    "print('total spend {} seconds'.format(int(time.time()-starttime)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【create source model】\n",
      "domain number: 4\n",
      "tag size: 4\n",
      "use crf? True\n",
      "use BERT? True\n",
      "maximum epochs: 5\n",
      "datasize per batch: 50\n",
      "no opt. max times: 2\n",
      "start to train DANN!\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7746db9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7746db9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f77467dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f77467dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 in batch 0 spend 138\n",
      "label loss:311.48398; label acc.:0.095\n",
      "adv. loss:2.18905; adv. acc.:0.22\n",
      "\n",
      "epoch 0 in batch 1 spend 68\n",
      "label loss:302.11102; label acc.:0.415\n",
      "adv. loss:4.59204; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 2 spend 68\n",
      "label loss:239.79359; label acc.:0.16\n",
      "adv. loss:2.31555; adv. acc.:0.168\n",
      "\n",
      "epoch 0 in batch 3 spend 67\n",
      "label loss:220.27699; label acc.:0.265\n",
      "adv. loss:9.90711; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 4 spend 67\n",
      "label loss:198.94414; label acc.:0.235\n",
      "adv. loss:6.69642; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 5 spend 69\n",
      "label loss:200.56319; label acc.:0.26\n",
      "adv. loss:11.24656; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 6 spend 69\n",
      "label loss:227.44557; label acc.:0.205\n",
      "adv. loss:13.20655; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 7 spend 69\n",
      "label loss:211.23969; label acc.:0.45\n",
      "adv. loss:15.58029; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 8 spend 69\n",
      "label loss:187.51132; label acc.:0.445\n",
      "adv. loss:20.25161; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 9 spend 68\n",
      "label loss:162.62886; label acc.:0.455\n",
      "adv. loss:25.10936; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 10 spend 68\n",
      "label loss:141.13281; label acc.:0.24\n",
      "adv. loss:24.53767; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 11 spend 68\n",
      "label loss:131.20403; label acc.:0.435\n",
      "adv. loss:25.44315; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 12 spend 68\n",
      "label loss:122.19785; label acc.:0.45\n",
      "adv. loss:25.49431; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 13 spend 68\n",
      "label loss:114.35414; label acc.:0.215\n",
      "adv. loss:24.81112; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 14 spend 68\n",
      "label loss:106.22861; label acc.:0.415\n",
      "adv. loss:23.9827; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 15 spend 68\n",
      "label loss:96.47367; label acc.:0.6\n",
      "adv. loss:23.11259; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 16 spend 68\n",
      "label loss:87.29216; label acc.:0.825\n",
      "adv. loss:22.21228; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 17 spend 68\n",
      "label loss:80.92569; label acc.:0.57\n",
      "adv. loss:21.25385; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 18 spend 68\n",
      "label loss:73.04243; label acc.:0.64\n",
      "adv. loss:20.2685; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 19 spend 68\n",
      "label loss:65.81364; label acc.:0.645\n",
      "adv. loss:19.22776; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 20 spend 68\n",
      "label loss:60.02794; label acc.:0.61\n",
      "adv. loss:18.15952; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 21 spend 68\n",
      "label loss:54.86555; label acc.:0.835\n",
      "adv. loss:17.09059; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 22 spend 68\n",
      "label loss:53.20236; label acc.:0.395\n",
      "adv. loss:16.01303; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 23 spend 68\n",
      "label loss:47.29716; label acc.:0.88\n",
      "adv. loss:14.92945; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 24 spend 68\n",
      "label loss:46.19224; label acc.:0.865\n",
      "adv. loss:13.85592; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 25 spend 68\n",
      "label loss:393.89188; label acc.:0.435\n",
      "adv. loss:5.71526; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 26 spend 68\n",
      "label loss:373.84546; label acc.:0.45\n",
      "adv. loss:5.63071; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 27 spend 68\n",
      "label loss:351.34854; label acc.:0.47\n",
      "adv. loss:5.81223; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 28 spend 68\n",
      "label loss:328.08777; label acc.:0.25\n",
      "adv. loss:6.06061; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 29 spend 68\n",
      "label loss:308.74313; label acc.:0.45\n",
      "adv. loss:6.19726; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 30 spend 68\n",
      "label loss:291.3396; label acc.:0.26\n",
      "adv. loss:6.45429; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 31 spend 68\n",
      "label loss:274.42862; label acc.:0.26\n",
      "adv. loss:6.56617; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 32 spend 67\n",
      "label loss:257.40372; label acc.:0.465\n",
      "adv. loss:6.52212; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 33 spend 69\n",
      "label loss:242.11487; label acc.:0.4\n",
      "adv. loss:6.33095; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 34 spend 68\n",
      "label loss:227.60112; label acc.:0.41\n",
      "adv. loss:6.11025; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 35 spend 68\n",
      "label loss:215.39915; label acc.:0.0\n",
      "adv. loss:5.86239; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 36 spend 68\n",
      "label loss:203.27573; label acc.:0.24\n",
      "adv. loss:5.7408; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 37 spend 68\n",
      "label loss:191.81348; label acc.:0.24\n",
      "adv. loss:5.83095; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 38 spend 68\n",
      "label loss:183.61739; label acc.:0.245\n",
      "adv. loss:6.08674; adv. acc.:0.2\n",
      "\n",
      "epoch 0 in batch 39 spend 68\n",
      "label loss:178.17125; label acc.:0.2\n",
      "adv. loss:6.33813; adv. acc.:0.2\n",
      "\n",
      "#epoch  0\n",
      "label loss:272.30197; label acc.:0.4095; spend 2810 sec.\n",
      "adv. loss:12.81865; adv. acc.:0.1997; spend 2810 sec.\n",
      "------------------------------\n",
      "epoch 1 in batch 0 spend 68\n",
      "label loss:171.93626; label acc.:0.245\n",
      "adv. loss:6.78422; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 1 spend 68\n",
      "label loss:166.35312; label acc.:0.405\n",
      "adv. loss:7.33944; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 2 spend 68\n",
      "label loss:157.57988; label acc.:0.415\n",
      "adv. loss:8.11063; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 3 spend 68\n",
      "label loss:149.41251; label acc.:0.41\n",
      "adv. loss:9.74516; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 4 spend 67\n",
      "label loss:144.30803; label acc.:0.6\n",
      "adv. loss:12.74203; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 5 spend 68\n",
      "label loss:141.89891; label acc.:0.44\n",
      "adv. loss:14.86652; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 6 spend 68\n",
      "label loss:139.46045; label acc.:0.88\n",
      "adv. loss:16.43482; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 7 spend 68\n",
      "label loss:132.40558; label acc.:0.57\n",
      "adv. loss:18.12035; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 8 spend 67\n",
      "label loss:120.4306; label acc.:0.605\n",
      "adv. loss:19.4153; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 9 spend 68\n",
      "label loss:113.6292; label acc.:0.64\n",
      "adv. loss:20.39931; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 10 spend 68\n",
      "label loss:102.92114; label acc.:0.615\n",
      "adv. loss:21.04701; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 11 spend 68\n",
      "label loss:96.16518; label acc.:0.63\n",
      "adv. loss:21.12829; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 12 spend 68\n",
      "label loss:90.6843; label acc.:0.86\n",
      "adv. loss:20.88824; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 13 spend 68\n",
      "label loss:81.79308; label acc.:0.85\n",
      "adv. loss:20.53279; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 14 spend 68\n",
      "label loss:76.26581; label acc.:0.865\n",
      "adv. loss:20.08415; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 15 spend 68\n",
      "label loss:71.9975; label acc.:0.88\n",
      "adv. loss:19.57326; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 16 spend 68\n",
      "label loss:65.62624; label acc.:0.845\n",
      "adv. loss:18.96957; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 17 spend 68\n",
      "label loss:62.08908; label acc.:0.86\n",
      "adv. loss:18.22997; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 18 spend 68\n",
      "label loss:59.79871; label acc.:0.835\n",
      "adv. loss:17.4248; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 19 spend 67\n",
      "label loss:57.0127; label acc.:0.885\n",
      "adv. loss:16.57827; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 20 spend 69\n",
      "label loss:54.19504; label acc.:0.865\n",
      "adv. loss:15.71457; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 21 spend 67\n",
      "label loss:51.14846; label acc.:0.86\n",
      "adv. loss:14.82193; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 22 spend 68\n",
      "label loss:49.1498; label acc.:0.87\n",
      "adv. loss:13.9181; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 23 spend 67\n",
      "label loss:45.40892; label acc.:0.815\n",
      "adv. loss:12.99612; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 24 spend 68\n",
      "label loss:44.33265; label acc.:0.865\n",
      "adv. loss:12.05683; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 25 spend 68\n",
      "label loss:43.58879; label acc.:0.84\n",
      "adv. loss:11.0755; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 26 spend 68\n",
      "label loss:41.66042; label acc.:0.83\n",
      "adv. loss:10.03869; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 27 spend 69\n",
      "label loss:39.57433; label acc.:0.82\n",
      "adv. loss:8.97305; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 28 spend 70\n",
      "label loss:39.48017; label acc.:0.875\n",
      "adv. loss:7.89002; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 29 spend 69\n",
      "label loss:36.80181; label acc.:0.875\n",
      "adv. loss:6.79854; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 30 spend 70\n",
      "label loss:35.77129; label acc.:0.86\n",
      "adv. loss:5.70876; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 31 spend 68\n",
      "label loss:31.77114; label acc.:0.845\n",
      "adv. loss:4.63143; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 32 spend 68\n",
      "label loss:33.721; label acc.:0.86\n",
      "adv. loss:3.60502; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 33 spend 68\n",
      "label loss:32.29884; label acc.:0.825\n",
      "adv. loss:2.80208; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 34 spend 69\n",
      "label loss:34.23954; label acc.:0.83\n",
      "adv. loss:2.4229; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 35 spend 68\n",
      "label loss:30.0489; label acc.:0.855\n",
      "adv. loss:2.40928; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 36 spend 69\n",
      "label loss:32.17062; label acc.:0.86\n",
      "adv. loss:2.49634; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 37 spend 69\n",
      "label loss:30.50207; label acc.:0.86\n",
      "adv. loss:2.56214; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 38 spend 69\n",
      "label loss:28.55203; label acc.:0.825\n",
      "adv. loss:2.5624; adv. acc.:0.2\n",
      "\n",
      "epoch 1 in batch 39 spend 69\n",
      "label loss:31.43533; label acc.:0.865\n",
      "adv. loss:2.52171; adv. acc.:0.2\n",
      "\n",
      "#epoch  1\n",
      "label loss:96.48962; label acc.:0.75838; spend 2743 sec.\n",
      "adv. loss:11.86049; adv. acc.:0.2; spend 2743 sec.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 in batch 0 spend 69\n",
      "label loss:32.38158; label acc.:0.835\n",
      "adv. loss:2.4789; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 1 spend 69\n",
      "label loss:28.67718; label acc.:0.875\n",
      "adv. loss:2.46108; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 2 spend 72\n",
      "label loss:28.29774; label acc.:0.865\n",
      "adv. loss:2.50388; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 3 spend 74\n",
      "label loss:25.81173; label acc.:0.85\n",
      "adv. loss:2.61372; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 4 spend 75\n",
      "label loss:28.05194; label acc.:0.87\n",
      "adv. loss:2.73051; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 5 spend 74\n",
      "label loss:29.65903; label acc.:0.84\n",
      "adv. loss:2.78554; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 6 spend 73\n",
      "label loss:29.12881; label acc.:0.885\n",
      "adv. loss:2.75606; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 7 spend 72\n",
      "label loss:30.59589; label acc.:0.845\n",
      "adv. loss:2.67766; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 8 spend 73\n",
      "label loss:27.49424; label acc.:0.875\n",
      "adv. loss:2.60048; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 9 spend 73\n",
      "label loss:28.2778; label acc.:0.87\n",
      "adv. loss:2.55974; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 10 spend 73\n",
      "label loss:27.49854; label acc.:0.85\n",
      "adv. loss:2.55603; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 11 spend 68\n",
      "label loss:28.84678; label acc.:0.865\n",
      "adv. loss:2.56237; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 12 spend 67\n",
      "label loss:27.0421; label acc.:0.825\n",
      "adv. loss:2.55728; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 13 spend 68\n",
      "label loss:26.87605; label acc.:0.86\n",
      "adv. loss:2.53277; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 14 spend 67\n",
      "label loss:29.24262; label acc.:0.86\n",
      "adv. loss:2.49149; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 15 spend 68\n",
      "label loss:29.2409; label acc.:0.89\n",
      "adv. loss:2.43945; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 16 spend 68\n",
      "label loss:30.4968; label acc.:0.905\n",
      "adv. loss:2.38085; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 17 spend 68\n",
      "label loss:33.3619; label acc.:0.85\n",
      "adv. loss:2.31533; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 18 spend 68\n",
      "label loss:25.50546; label acc.:0.83\n",
      "adv. loss:2.24152; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 19 spend 68\n",
      "label loss:26.30975; label acc.:0.855\n",
      "adv. loss:2.15991; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 20 spend 68\n",
      "label loss:29.01301; label acc.:0.84\n",
      "adv. loss:2.07467; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 21 spend 67\n",
      "label loss:29.04362; label acc.:0.88\n",
      "adv. loss:1.9924; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 22 spend 68\n",
      "label loss:25.75959; label acc.:0.85\n",
      "adv. loss:1.92001; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 23 spend 68\n",
      "label loss:27.09887; label acc.:0.85\n",
      "adv. loss:1.86204; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 24 spend 68\n",
      "label loss:30.03629; label acc.:0.82\n",
      "adv. loss:1.81697; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 25 spend 68\n",
      "label loss:27.1384; label acc.:0.875\n",
      "adv. loss:1.77866; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 26 spend 68\n",
      "label loss:25.0569; label acc.:0.83\n",
      "adv. loss:1.74035; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 27 spend 68\n",
      "label loss:25.6542; label acc.:0.86\n",
      "adv. loss:1.6999; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 28 spend 68\n",
      "label loss:27.31605; label acc.:0.875\n",
      "adv. loss:1.66191; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 29 spend 68\n",
      "label loss:29.1711; label acc.:0.855\n",
      "adv. loss:1.63478; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 30 spend 68\n",
      "label loss:27.64222; label acc.:0.805\n",
      "adv. loss:1.62523; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 31 spend 67\n",
      "label loss:22.72717; label acc.:0.85\n",
      "adv. loss:1.63435; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 32 spend 68\n",
      "label loss:26.77501; label acc.:0.865\n",
      "adv. loss:1.65658; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 33 spend 68\n",
      "label loss:29.98966; label acc.:0.895\n",
      "adv. loss:1.68294; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 34 spend 68\n",
      "label loss:28.32538; label acc.:0.84\n",
      "adv. loss:1.70505; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 35 spend 68\n",
      "label loss:28.74044; label acc.:0.825\n",
      "adv. loss:1.71716; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 36 spend 68\n",
      "label loss:25.49309; label acc.:0.84\n",
      "adv. loss:1.71737; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 37 spend 68\n",
      "label loss:29.38456; label acc.:0.81\n",
      "adv. loss:1.707; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 38 spend 68\n",
      "label loss:27.72977; label acc.:0.82\n",
      "adv. loss:1.68955; adv. acc.:0.2\n",
      "\n",
      "epoch 2 in batch 39 spend 68\n",
      "label loss:28.73879; label acc.:0.845\n",
      "adv. loss:1.66895; adv. acc.:0.2\n",
      "\n",
      "#epoch  2\n",
      "label loss:31.30403; label acc.:0.85325; spend 2783 sec.\n",
      "adv. loss:2.13476; adv. acc.:0.2; spend 2783 sec.\n",
      "------------------------------\n",
      "epoch 3 in batch 0 spend 67\n",
      "label loss:26.37416; label acc.:0.855\n",
      "adv. loss:1.69235; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 1 spend 67\n",
      "label loss:28.20411; label acc.:0.89\n",
      "adv. loss:1.63259; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 2 spend 68\n",
      "label loss:28.08936; label acc.:0.855\n",
      "adv. loss:1.62081; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 3 spend 68\n",
      "label loss:27.3758; label acc.:0.865\n",
      "adv. loss:1.61446; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 4 spend 68\n",
      "label loss:27.65884; label acc.:0.84\n",
      "adv. loss:1.61302; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 5 spend 68\n",
      "label loss:26.91698; label acc.:0.855\n",
      "adv. loss:1.61563; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 6 spend 68\n",
      "label loss:28.02708; label acc.:0.885\n",
      "adv. loss:1.62068; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 7 spend 68\n",
      "label loss:27.28644; label acc.:0.825\n",
      "adv. loss:1.62619; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 8 spend 68\n",
      "label loss:29.6947; label acc.:0.885\n",
      "adv. loss:1.63048; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 9 spend 67\n",
      "label loss:24.60294; label acc.:0.825\n",
      "adv. loss:1.63234; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 10 spend 68\n",
      "label loss:25.96985; label acc.:0.885\n",
      "adv. loss:1.63176; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 11 spend 68\n",
      "label loss:31.03131; label acc.:0.86\n",
      "adv. loss:1.6295; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 12 spend 67\n",
      "label loss:30.45387; label acc.:0.87\n",
      "adv. loss:1.62624; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 13 spend 68\n",
      "label loss:27.56382; label acc.:0.86\n",
      "adv. loss:1.62303; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 14 spend 67\n",
      "label loss:26.26806; label acc.:0.84\n",
      "adv. loss:1.62044; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 15 spend 67\n",
      "label loss:25.74291; label acc.:0.885\n",
      "adv. loss:1.61855; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 16 spend 68\n",
      "label loss:27.99255; label acc.:0.835\n",
      "adv. loss:1.61708; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 17 spend 67\n",
      "label loss:29.77679; label acc.:0.875\n",
      "adv. loss:1.61599; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 18 spend 67\n",
      "label loss:31.86126; label acc.:0.86\n",
      "adv. loss:1.615; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 19 spend 68\n",
      "label loss:25.13015; label acc.:0.865\n",
      "adv. loss:1.61389; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 20 spend 68\n",
      "label loss:27.29496; label acc.:0.86\n",
      "adv. loss:1.61303; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 21 spend 68\n",
      "label loss:29.87035; label acc.:0.86\n",
      "adv. loss:1.61222; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 22 spend 68\n",
      "label loss:25.43511; label acc.:0.855\n",
      "adv. loss:1.61155; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 23 spend 68\n",
      "label loss:27.20215; label acc.:0.89\n",
      "adv. loss:1.61139; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 24 spend 68\n",
      "label loss:29.42629; label acc.:0.845\n",
      "adv. loss:1.61145; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 25 spend 68\n",
      "label loss:30.48697; label acc.:0.845\n",
      "adv. loss:1.61172; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 26 spend 67\n",
      "label loss:25.78985; label acc.:0.88\n",
      "adv. loss:1.61205; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 27 spend 68\n",
      "label loss:27.84971; label acc.:0.86\n",
      "adv. loss:1.61251; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 28 spend 67\n",
      "label loss:26.41055; label acc.:0.82\n",
      "adv. loss:1.61275; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 29 spend 68\n",
      "label loss:26.9746; label acc.:0.825\n",
      "adv. loss:1.61272; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 30 spend 68\n",
      "label loss:28.86537; label acc.:0.88\n",
      "adv. loss:1.61241; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 31 spend 68\n",
      "label loss:23.91858; label acc.:0.845\n",
      "adv. loss:1.61192; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 32 spend 68\n",
      "label loss:24.7794; label acc.:0.82\n",
      "adv. loss:1.61125; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 33 spend 68\n",
      "label loss:26.33941; label acc.:0.815\n",
      "adv. loss:1.61066; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 34 spend 68\n",
      "label loss:26.77193; label acc.:0.825\n",
      "adv. loss:1.61027; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 35 spend 68\n",
      "label loss:28.62595; label acc.:0.85\n",
      "adv. loss:1.60993; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 36 spend 67\n",
      "label loss:25.54216; label acc.:0.87\n",
      "adv. loss:1.60976; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 37 spend 68\n",
      "label loss:24.50669; label acc.:0.835\n",
      "adv. loss:1.6098; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 38 spend 67\n",
      "label loss:27.587; label acc.:0.825\n",
      "adv. loss:1.60979; adv. acc.:0.2\n",
      "\n",
      "epoch 3 in batch 39 spend 68\n",
      "label loss:28.48022; label acc.:0.87\n",
      "adv. loss:1.6099; adv. acc.:0.2\n",
      "\n",
      "#epoch  3\n",
      "label loss:25.65265; label acc.:0.85475; spend 2724 sec.\n",
      "adv. loss:1.61843; adv. acc.:0.2; spend 2724 sec.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 in batch 0 spend 67\n",
      "label loss:24.28326; label acc.:0.87\n",
      "adv. loss:1.61002; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 1 spend 68\n",
      "label loss:27.0445; label acc.:0.845\n",
      "adv. loss:1.60995; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 2 spend 68\n",
      "label loss:27.77853; label acc.:0.87\n",
      "adv. loss:1.61014; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 3 spend 68\n",
      "label loss:28.65083; label acc.:0.855\n",
      "adv. loss:1.6101; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 4 spend 68\n",
      "label loss:26.35558; label acc.:0.855\n",
      "adv. loss:1.61013; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 5 spend 68\n",
      "label loss:27.21803; label acc.:0.845\n",
      "adv. loss:1.61007; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 6 spend 68\n",
      "label loss:24.81693; label acc.:0.885\n",
      "adv. loss:1.60997; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 7 spend 68\n",
      "label loss:26.39206; label acc.:0.905\n",
      "adv. loss:1.60988; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 8 spend 68\n",
      "label loss:28.37292; label acc.:0.86\n",
      "adv. loss:1.60978; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 9 spend 68\n",
      "label loss:26.62097; label acc.:0.88\n",
      "adv. loss:1.60956; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 10 spend 68\n",
      "label loss:27.14259; label acc.:0.89\n",
      "adv. loss:1.60957; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 11 spend 68\n",
      "label loss:26.19813; label acc.:0.915\n",
      "adv. loss:1.60948; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 12 spend 68\n",
      "label loss:29.48106; label acc.:0.825\n",
      "adv. loss:1.60949; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 13 spend 68\n",
      "label loss:25.41784; label acc.:0.85\n",
      "adv. loss:1.60944; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 14 spend 68\n",
      "label loss:27.48755; label acc.:0.87\n",
      "adv. loss:1.60949; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 15 spend 68\n",
      "label loss:27.36723; label acc.:0.84\n",
      "adv. loss:1.60478; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 16 spend 68\n",
      "label loss:27.52351; label acc.:0.855\n",
      "adv. loss:1.60958; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 17 spend 68\n",
      "label loss:27.58186; label acc.:0.79\n",
      "adv. loss:1.60957; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 18 spend 68\n",
      "label loss:29.20791; label acc.:0.84\n",
      "adv. loss:1.60677; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 19 spend 67\n",
      "label loss:26.49689; label acc.:0.89\n",
      "adv. loss:1.60963; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 20 spend 68\n",
      "label loss:26.49993; label acc.:0.865\n",
      "adv. loss:1.60956; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 21 spend 68\n",
      "label loss:28.50215; label acc.:0.845\n",
      "adv. loss:1.60951; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 22 spend 68\n",
      "label loss:27.32704; label acc.:0.84\n",
      "adv. loss:1.60949; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 23 spend 68\n",
      "label loss:26.54297; label acc.:0.82\n",
      "adv. loss:1.60949; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 24 spend 68\n",
      "label loss:27.52161; label acc.:0.825\n",
      "adv. loss:1.60951; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 25 spend 68\n",
      "label loss:27.02994; label acc.:0.84\n",
      "adv. loss:1.60945; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 26 spend 68\n",
      "label loss:25.39833; label acc.:0.885\n",
      "adv. loss:1.60941; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 27 spend 67\n",
      "label loss:31.30979; label acc.:0.855\n",
      "adv. loss:1.60947; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 28 spend 68\n",
      "label loss:31.20562; label acc.:0.805\n",
      "adv. loss:1.60952; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 29 spend 68\n",
      "label loss:25.8003; label acc.:0.84\n",
      "adv. loss:1.60941; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 30 spend 68\n",
      "label loss:29.35608; label acc.:0.78\n",
      "adv. loss:1.60946; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 31 spend 68\n",
      "label loss:25.27989; label acc.:0.845\n",
      "adv. loss:1.60949; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 32 spend 68\n",
      "label loss:25.41433; label acc.:0.88\n",
      "adv. loss:1.6095; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 33 spend 68\n",
      "label loss:25.76606; label acc.:0.87\n",
      "adv. loss:1.60945; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 34 spend 68\n",
      "label loss:28.66868; label acc.:0.845\n",
      "adv. loss:1.60938; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 35 spend 68\n",
      "label loss:25.08991; label acc.:0.84\n",
      "adv. loss:1.60943; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 36 spend 68\n",
      "label loss:28.01521; label acc.:0.845\n",
      "adv. loss:1.60942; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 37 spend 68\n",
      "label loss:28.32384; label acc.:0.865\n",
      "adv. loss:1.60943; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 38 spend 68\n",
      "label loss:23.53202; label acc.:0.84\n",
      "adv. loss:1.60945; adv. acc.:0.2\n",
      "\n",
      "epoch 4 in batch 39 spend 68\n",
      "label loss:25.20013; label acc.:0.85\n",
      "adv. loss:1.60942; adv. acc.:0.2\n",
      "\n",
      "#epoch  4\n",
      "label loss:24.70697; label acc.:0.85287; spend 2728 sec.\n",
      "adv. loss:1.60942; adv. acc.:0.2; spend 2728 sec.\n",
      "------------------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#train on batch\n",
    "#DANN training 的時候用的target x 是train_x; 模型訓練完後還是要像meta learning一樣再去對模型finetune\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "epochs = 5\n",
    "datasize_per_domain = 50\n",
    "update_times =2\n",
    "filt_sentence = False\n",
    "my_tags = [msra_tags, people_tags, weibo_tags, singer_tags]\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    data_list = [filter_sentence(msra_train_x_bert, msra_train_y_bert, msra_idx2label,filt_sentence), \n",
    "                filter_sentence(people_train_x_bert, people_train_y_bert, people_idx2label,filt_sentence), \n",
    "                filter_sentence(weibo_train_x_bert, weibo_train_y_bert, weibo_idx2label, filt_sentence), \n",
    "                filter_sentence(singer_train_x_bert, singer_train_y_bert, singer_idx2label, filt_sentence),\n",
    "                filter_sentence(homeapp_train_x_bert, homeapp_train_y_bert, homeapp_idx2label, filt_sentence)]\n",
    "    tmp_model = create_src_model(my_tags, crf_layer=True, compile_ = True, use_bert=True)\n",
    "\n",
    "print('maximum epochs:', epochs)\n",
    "print('datasize per batch:', datasize_per_domain)\n",
    "print('no opt. max times:', update_times)\n",
    "print('start to train DANN!')\n",
    "starttime = time.time()\n",
    "flag_count = 0\n",
    "best_loss = 1000.0\n",
    "for epoch in range(epochs):\n",
    "    #對於每個batch，都有每個domain的資訊 e.g. 3個src, 1個tgt, 一個batch可能就有40筆資料，每個domain各10筆\n",
    "    #要分tgt domain 有/無 label的case    \n",
    "    loss, adv_loss = [], []\n",
    "    acc, adv_acc = [], []\n",
    "    #in each batch\n",
    "    for batches in range(int(len(data_list[-1][1])/datasize_per_domain)+1):\n",
    "#     for batches in range(1):\n",
    "        start_ = time.time()\n",
    "        start = batches*datasize_per_domain\n",
    "        x, _, domain, x_wo_tgt, label_wo_tgt = sample_data_dann(data_list, datasize_per_domain, start_=start)\n",
    "        if len(domain[0])==len(data_list):\n",
    "            #先對每一個domain的 NER訓練\n",
    "            for dm in range(len(label_wo_tgt)):\n",
    "                #預測一波\n",
    "                pred_label = tmp_model['decoding_model'][dm].predict(x_wo_tgt[dm])\n",
    "                pred_label = [np.argmax(i) for i in pred_label]\n",
    "                ans_label = [np.argmax(i) for i in label_wo_tgt[dm]]\n",
    "                acc.append(accuracy_score(ans_label, pred_label))\n",
    "                #再訓練\n",
    "                with tf.device('/cpu:0'): #這邊不用cpu跑會OOM\n",
    "                    tmp_loss = tmp_model['decoding_model'][dm].train_on_batch(x=x_wo_tgt[dm], y=label_wo_tgt[dm])[0]\n",
    "                loss.append(round(tmp_loss, 5))\n",
    "\n",
    "            #再對domain discriminator進行訓練(一起訓練)\n",
    "            pred_domain = tmp_model['discriminator_model'].predict(x)\n",
    "            with tf.device('/cpu:0'): #這邊不用cpu跑會OOM\n",
    "                tmp_adv_loss = tmp_model['discriminator_model'].train_on_batch(x=x, y=domain)[0]        \n",
    "            adv_loss.append(round(tmp_adv_loss, 5))\n",
    "            pred_domain = [np.argmax(i) for i in pred_domain]\n",
    "            ans_domain = [np.argmax(i) for i in domain]\n",
    "            adv_acc.append(accuracy_score(ans_domain, pred_domain))\n",
    "            print('epoch {} in batch'.format(epoch), batches, 'spend', int(time.time()-start_))        \n",
    "            print('label loss:{}; label acc.:{}'.format(round(tmp_loss, 5), round(np.mean(acc[-len(label_wo_tgt):]), 5)))\n",
    "            print('adv. loss:{}; adv. acc.:{}'.format(round(tmp_adv_loss, 5), round(adv_acc[-1], 5)))\n",
    "            print()\n",
    "            \n",
    "    print('#epoch ', epoch)\n",
    "    print('label loss:{}; label acc.:{}; spend {} sec.'.format(round(np.mean(loss), 5), round(np.mean(acc), 5), int(time.time()-starttime)))\n",
    "    print('adv. loss:{}; adv. acc.:{}; spend {} sec.'.format(round(np.mean(adv_loss), 5), round(np.mean(adv_acc), 5), int(time.time()-starttime)))\n",
    "    print('------------------------------')\n",
    "    \n",
    "#    tmp_model['decoding_model'][dm].save('./MetaNER_weight_save/DANN_decoding.h5')\n",
    "    model_weight = []\n",
    "    tmp_model['decoding_model'][0]\n",
    "    for layer in tmp_model['decoding_model'][0].layers:\n",
    "        model_weight.append(layer.get_weights())\n",
    "    np.save('./MetaNER_weight_save/dann_weight.npy', np.array(model_weight))\n",
    "    del model_weight\n",
    "    if np.mean(loss)>=best_loss:\n",
    "        flag_count+=1\n",
    "    else:\n",
    "        flag_count=0\n",
    "        best_loss=np.mean(loss)\n",
    "    if flag_count>=update_times:\n",
    "        break\n",
    "    starttime = time.time()\n",
    "# del tmp_model\n",
    "print('done!')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
