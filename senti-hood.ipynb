{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是在senti-hood這份TABSA上用 BERT-pari & adversarial reptile測試的程式\n",
    "\n",
    "效果沒有在論文中提及(因為也不是我們要討論的重點)，參考就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import json\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import xml.etree.ElementTree\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow_text as text \n",
    "import model.tokenization as tokenization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.layers import Input, Dense,Dropout,Embedding,LSTM,Bidirectional, Masking, TimeDistributed, Conv1D, MaxPooling1D, Flatten, concatenate, GRU\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "nltk.download('punkt')\n",
    "data_dir = './data/sentihood/'\n",
    "BERT_src = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "BERT_LAYER = hub.KerasLayer(BERT_src, trainable=False)\n",
    "VOCAB_FILE = BERT_LAYER.resolved_object.vocab_file.asset_path.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(VOCAB_FILE, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_idx(data, aspect2idx):\n",
    "    ret = []\n",
    "    for _, _, _, aspect, _ in data:\n",
    "        ret.append(aspect2idx[aspect])\n",
    "    assert len(data) == len(ret)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentihood_json(in_file):\n",
    "    with open(in_file) as f:\n",
    "        data = json.load(f)\n",
    "    ret = []\n",
    "    for d in data:\n",
    "        text = d['text']\n",
    "        sent_id = d['id']\n",
    "        opinions = []\n",
    "        targets = set()\n",
    "        for opinion in d['opinions']:\n",
    "            sentiment = opinion['sentiment']\n",
    "            aspect = opinion['aspect']\n",
    "            target_entity = opinion['target_entity']\n",
    "            targets.add(target_entity)\n",
    "            opinions.append((target_entity, aspect, sentiment))\n",
    "        ret.append((sent_id, text, opinions))\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(data, all_aspects, allow_none=True):\n",
    "    ret = []\n",
    "    for sent_id, text, opinions in data:\n",
    "        for target_entity, aspect, sentiment in opinions:\n",
    "            if aspect not in all_aspects:\n",
    "                continue\n",
    "            ret.append((sent_id, text, target_entity, aspect, sentiment))\n",
    "        assert 'LOCATION1' in text\n",
    "        targets = set(['LOCATION1'])\n",
    "        if 'LOCATION2' in text:\n",
    "            targets.add('LOCATION2')\n",
    "        for target in targets:\n",
    "            aspects = set([a for t, a, _ in opinions if t == target])\n",
    "            if allow_none:\n",
    "                none_aspects = [a for a in all_aspects if a not in aspects]\n",
    "                for aspect in none_aspects:\n",
    "                    ret.append((sent_id, text, target, aspect, 'None'))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    ret = []\n",
    "    for sent_id, text, target_entity, aspect, sentiment in data:\n",
    "        new_text = nltk.word_tokenize(text)\n",
    "        new_aspect = aspect.split('-')\n",
    "        ret.append((sent_id, new_text, target_entity, new_aspect, sentiment))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(data_dir, aspect2idx, allow_none=True):\n",
    "    in_file = os.path.join(data_dir, 'sentihood-train.json')\n",
    "    train = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-dev.json')\n",
    "    dev = parse_sentihood_json(in_file)\n",
    "    in_file = os.path.join(data_dir, 'sentihood-test.json')\n",
    "    test = parse_sentihood_json(in_file)\n",
    "    \n",
    "    train = convert_input(train, aspect2idx, allow_none)\n",
    "    train_aspect_idx = get_aspect_idx(train, aspect2idx)\n",
    "    train = tokenize(train)\n",
    "    \n",
    "    dev = convert_input(dev, aspect2idx, allow_none)\n",
    "    dev_aspect_idx = get_aspect_idx(dev, aspect2idx)\n",
    "    dev = tokenize(dev)\n",
    "    \n",
    "    test = convert_input(test, aspect2idx, allow_none)\n",
    "    test_aspect_idx = get_aspect_idx(test, aspect2idx)\n",
    "    test = tokenize(test)\n",
    "\n",
    "    return (train, train_aspect_idx), (dev, dev_aspect_idx), (test, test_aspect_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create NLI_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) =  15008\n",
      "len(val) =  3748\n",
      "len(test) =  7516\n"
     ]
    }
   ],
   "source": [
    "#create NLI_M\n",
    "aspect2idx = {\n",
    "    'general': 0,\n",
    "    'price': 1,\n",
    "    'transit-location': 2,\n",
    "    'safety': 3,\n",
    "}\n",
    "#id / sentence / target / aspect(s) / sentiment\n",
    "(train, train_aspect_idx), (val, val_aspect_idx), (test, test_aspect_idx) = load_task(data_dir, aspect2idx)\n",
    "print(\"len(train) = \", len(train))\n",
    "print(\"len(val) = \", len(val))\n",
    "print(\"len(test) = \", len(test))\n",
    "\n",
    "train.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "val.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "test.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "\n",
    "dir_path = data_dir+'bert-pair/'\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "with open(dir_path+\"train_NLI_M.tsv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tsentence1\\tsentence2\\tlabel\\n\")\n",
    "    for v in train:\n",
    "        f.write(str(v[0])+\"\\t\")\n",
    "        word=v[1][0].lower()\n",
    "        if word=='location1':f.write('location - 1')\n",
    "        elif word=='location2':f.write('location - 2')\n",
    "        elif word[0]=='\\'':f.write(\"\\' \"+word[1:])\n",
    "        else:f.write(word)\n",
    "        for i in range(1,len(v[1])):\n",
    "            word=v[1][i].lower()\n",
    "            f.write(\" \")\n",
    "            if word == 'location1':\n",
    "                f.write('location - 1')\n",
    "            elif word == 'location2':\n",
    "                f.write('location - 2')\n",
    "            elif word[0] == '\\'':\n",
    "                f.write(\"\\' \" + word[1:])\n",
    "            else:\n",
    "                f.write(word)\n",
    "        f.write(\"\\t\")\n",
    "        if v[2]=='LOCATION1':f.write('location - 1 - ')\n",
    "        if v[2]=='LOCATION2':f.write('location - 2 - ')\n",
    "        if len(v[3])==1:\n",
    "            f.write(v[3][0]+\"\\t\")\n",
    "        else:\n",
    "            f.write(\"transit location\\t\")\n",
    "        f.write(v[4]+\"\\n\")\n",
    "\n",
    "with open(dir_path+\"dev_NLI_M.tsv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tsentence1\\tsentence2\\tlabel\\n\")\n",
    "    for v in val:\n",
    "        f.write(str(v[0])+\"\\t\")\n",
    "        word=v[1][0].lower()\n",
    "        if word=='location1':f.write('location - 1')\n",
    "        elif word=='location2':f.write('location - 2')\n",
    "        elif word[0]=='\\'':f.write(\"\\' \"+word[1:])\n",
    "        else:f.write(word)\n",
    "        for i in range(1,len(v[1])):\n",
    "            word=v[1][i].lower()\n",
    "            f.write(\" \")\n",
    "            if word == 'location1':\n",
    "                f.write('location - 1')\n",
    "            elif word == 'location2':\n",
    "                f.write('location - 2')\n",
    "            elif word[0] == '\\'':\n",
    "                f.write(\"\\' \" + word[1:])\n",
    "            else:\n",
    "                f.write(word)\n",
    "        f.write(\"\\t\")\n",
    "        if v[2]=='LOCATION1':f.write('location - 1 - ')\n",
    "        if v[2]=='LOCATION2':f.write('location - 2 - ')\n",
    "        if len(v[3])==1:\n",
    "            f.write(v[3][0]+\"\\t\")\n",
    "        else:\n",
    "            f.write(\"transit location\\t\")\n",
    "        f.write(v[4]+\"\\n\")\n",
    "\n",
    "with open(dir_path+\"test_NLI_M.tsv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"id\\tsentence1\\tsentence2\\tlabel\\n\")\n",
    "    for v in test:\n",
    "        f.write(str(v[0])+\"\\t\")\n",
    "        word=v[1][0].lower()\n",
    "        if word=='location1':f.write('location - 1')\n",
    "        elif word=='location2':f.write('location - 2')\n",
    "        elif word[0]=='\\'':f.write(\"\\' \"+word[1:])\n",
    "        else:f.write(word)\n",
    "        for i in range(1,len(v[1])):\n",
    "            word=v[1][i].lower()\n",
    "            f.write(\" \")\n",
    "            if word == 'location1':\n",
    "                f.write('location - 1')\n",
    "            elif word == 'location2':\n",
    "                f.write('location - 2')\n",
    "            elif word[0] == '\\'':\n",
    "                f.write(\"\\' \" + word[1:])\n",
    "            else:\n",
    "                f.write(word)\n",
    "        f.write(\"\\t\")\n",
    "        if v[2]=='LOCATION1':f.write('location - 1 - ')\n",
    "        if v[2]=='LOCATION2':f.write('location - 2 - ')\n",
    "        if len(v[3])==1:\n",
    "            f.write(v[3][0]+\"\\t\")\n",
    "        else:\n",
    "            f.write(\"transit location\\t\")\n",
    "        f.write(v[4]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir='./data/sentihood/'\n",
    "aspect2idx = {\n",
    "    'general': 0,\n",
    "    'price': 1,\n",
    "    'transit-location': 2,\n",
    "    'safety': 3,\n",
    "}\n",
    "\n",
    "# (train, train_aspect_idx), (val, val_aspect_idx), (test, test_aspect_idx) = load_task(data_dir, aspect2idx)\n",
    "\n",
    "# print(\"len(train) = \", len(train))\n",
    "# print(\"len(val) = \", len(val))\n",
    "# print(\"len(test) = \", len(test))\n",
    "\n",
    "# train.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "# val.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "# test.sort(key=lambda x:x[2]+str(x[0])+x[3][0])\n",
    "\n",
    "location_name = ['loc1', 'loc2']\n",
    "aspect_name = ['general', 'price', 'safety', 'transit']\n",
    "dir_path = [data_dir + 'bert-single/' + i + '_' + j + '/' for i in location_name for j in aspect_name]\n",
    "for path in dir_path:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "count=0\n",
    "with open(dir_path[0]+\"train.tsv\",\"w\",encoding=\"utf-8\") as f1_general, \\\n",
    "    open(dir_path[1]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f1_price, \\\n",
    "    open(dir_path[2]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f1_safety, \\\n",
    "    open(dir_path[3]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f1_transit, \\\n",
    "    open(dir_path[4]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f2_general, \\\n",
    "    open(dir_path[5]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f2_price, \\\n",
    "    open(dir_path[6]+\"train.tsv\", \"w\", encoding=\"utf-8\") as f2_safety, \\\n",
    "    open(dir_path[7]+\"train.tsv\", \"w\",encoding=\"utf-8\") as f2_transit, \\\n",
    "    open(data_dir + \"bert-pair/train_NLI_M.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    s = f.readline().strip()\n",
    "    s = f.readline().strip()\n",
    "    while s:\n",
    "        count+=1\n",
    "        tmp=s.split(\"\\t\")\n",
    "        line=tmp[0]+\"\\t\"+tmp[1]+\"\\t\"+tmp[3]+\"\\n\"\n",
    "        if count<=11908:               #loc1\n",
    "            if count%4==1:\n",
    "                f1_general.write(line)\n",
    "            if count%4==2:\n",
    "                f1_price.write(line)\n",
    "            if count%4==3:\n",
    "                f1_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f1_transit.write(line)\n",
    "        else:                          #loc2\n",
    "            if count%4==1:\n",
    "                f2_general.write(line)\n",
    "            if count%4==2:\n",
    "                f2_price.write(line)\n",
    "            if count%4==3:\n",
    "                f2_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f2_transit.write(line)\n",
    "        s = f.readline().strip()\n",
    "\n",
    "count=0\n",
    "with open(dir_path[0]+\"dev.tsv\",\"w\",encoding=\"utf-8\") as f1_general, \\\n",
    "    open(dir_path[1]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f1_price, \\\n",
    "    open(dir_path[2]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f1_safety, \\\n",
    "    open(dir_path[3]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f1_transit, \\\n",
    "    open(dir_path[4]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f2_general, \\\n",
    "    open(dir_path[5]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f2_price, \\\n",
    "    open(dir_path[6]+\"dev.tsv\", \"w\", encoding=\"utf-8\") as f2_safety, \\\n",
    "    open(dir_path[7]+\"dev.tsv\", \"w\",encoding=\"utf-8\") as f2_transit, \\\n",
    "    open(data_dir + \"bert-pair/dev_NLI_M.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    s = f.readline().strip()\n",
    "    s = f.readline().strip()\n",
    "    while s:\n",
    "        count+=1\n",
    "        tmp=s.split(\"\\t\")\n",
    "        line=tmp[0]+\"\\t\"+tmp[1]+\"\\t\"+tmp[3]+\"\\n\"\n",
    "        if count<=2988:               #loc1\n",
    "            if count%4==1:\n",
    "                f1_general.write(line)\n",
    "            if count%4==2:\n",
    "                f1_price.write(line)\n",
    "            if count%4==3:\n",
    "                f1_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f1_transit.write(line)\n",
    "        else:                          #loc2\n",
    "            if count%4==1:\n",
    "                f2_general.write(line)\n",
    "            if count%4==2:\n",
    "                f2_price.write(line)\n",
    "            if count%4==3:\n",
    "                f2_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f2_transit.write(line)\n",
    "        s = f.readline().strip()\n",
    "\n",
    "count=0\n",
    "with open(dir_path[0]+\"test.tsv\",\"w\",encoding=\"utf-8\") as f1_general, \\\n",
    "    open(dir_path[1]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f1_price, \\\n",
    "    open(dir_path[2]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f1_safety, \\\n",
    "    open(dir_path[3]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f1_transit, \\\n",
    "    open(dir_path[4]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f2_general, \\\n",
    "    open(dir_path[5]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f2_price, \\\n",
    "    open(dir_path[6]+\"test.tsv\", \"w\", encoding=\"utf-8\") as f2_safety, \\\n",
    "    open(dir_path[7]+\"test.tsv\", \"w\",encoding=\"utf-8\") as f2_transit, \\\n",
    "    open(data_dir + \"bert-pair/test_NLI_M.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    s = f.readline().strip()\n",
    "    s = f.readline().strip()\n",
    "    while s:\n",
    "        count+=1\n",
    "        tmp=s.split(\"\\t\")\n",
    "        line=tmp[0]+\"\\t\"+tmp[1]+\"\\t\"+tmp[3]+\"\\n\"\n",
    "        if count<=5964:               #loc1\n",
    "            if count%4==1:\n",
    "                f1_general.write(line)\n",
    "            if count%4==2:\n",
    "                f1_price.write(line)\n",
    "            if count%4==3:\n",
    "                f1_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f1_transit.write(line)\n",
    "        else:                          #loc2\n",
    "            if count%4==1:\n",
    "                f2_general.write(line)\n",
    "            if count%4==2:\n",
    "                f2_price.write(line)\n",
    "            if count%4==3:\n",
    "                f2_safety.write(line)\n",
    "            if count%4==0:\n",
    "                f2_transit.write(line)\n",
    "        s = f.readline().strip()\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "        \n",
    "class Sentihood_single_Processor(DataProcessor):\n",
    "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        train_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"),header=None,sep=\"\\t\").values\n",
    "        return self._create_examples(train_data, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev.tsv\"),header=None,sep=\"\\t\").values\n",
    "        return self._create_examples(dev_data, \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        test_data = pd.read_csv(os.path.join(data_dir, \"test.tsv\"),header=None,sep=\"\\t\").values\n",
    "        return self._create_examples(test_data, \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return ['None', 'Positive', 'Negative']\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "          #  if i>50:break\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(str(line[1]))\n",
    "            label = tokenization.convert_to_unicode(str(line[2]))\n",
    "#             if i%1000==0:\n",
    "#                 print(i)\n",
    "#                 print(\"guid=\",guid)\n",
    "#                 print(\"text_a=\",text_a)\n",
    "# #                 print(\"text_b=\",text_b)\n",
    "#                 print(\"label=\",label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "    \n",
    "class Sentihood_NLI_M_Processor(DataProcessor):\n",
    "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        train_data = pd.read_csv(os.path.join(data_dir, \"train_NLI_M.tsv\"),sep=\"\\t\").values\n",
    "        return self._create_examples(train_data, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        dev_data = pd.read_csv(os.path.join(data_dir, \"dev_NLI_M.tsv\"),sep=\"\\t\").values\n",
    "        return self._create_examples(dev_data, \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        test_data = pd.read_csv(os.path.join(data_dir, \"test_NLI_M.tsv\"),sep=\"\\t\").values\n",
    "        return self._create_examples(test_data, \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return ['None', 'Positive', 'Negative']\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "          #  if i>50:break\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(str(line[1]))\n",
    "            text_b = tokenization.convert_to_unicode(str(line[2]))\n",
    "            label = tokenization.convert_to_unicode(str(line[3]))\n",
    "#             if i%1000==0:\n",
    "#                 print(i)\n",
    "#                 print(\"guid=\",guid)\n",
    "#                 print(\"text_a=\",text_a)\n",
    "#                 print(\"text_b=\",text_b)\n",
    "#                 print(\"label=\",label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "    \n",
    "# processor = Sentihood_single_Processor()\n",
    "processor = Sentihood_NLI_M_Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentihood_macro_F1(y_true_, y_pred_):\n",
    "    \"\"\"\n",
    "    Calculate \"Macro-F1\" of aspect detection task of Sentihood.\n",
    "    \"\"\"\n",
    "    sent2idx = {'無':0, '正向':1, '負向':2}\n",
    "    y_true = [sent2idx[i] for i in y_true_]\n",
    "    y_pred = [sent2idx[i] for i in y_pred_]\n",
    "    p_all=0\n",
    "    r_all=0\n",
    "    count=0\n",
    "    for i in range(len(y_pred)//4):\n",
    "        a=set()\n",
    "        b=set()\n",
    "        for j in range(4):\n",
    "            if y_pred[i*4+j]!=0:\n",
    "                a.add(j)\n",
    "            if y_true[i*4+j]!=0:\n",
    "                b.add(j)\n",
    "        if len(b)==0:continue\n",
    "        a_b=a.intersection(b)\n",
    "        if len(a_b)>0:\n",
    "            p=len(a_b)/len(a)\n",
    "            r=len(a_b)/len(b)\n",
    "        else:\n",
    "            p=0\n",
    "            r=0\n",
    "        count+=1\n",
    "        p_all+=p\n",
    "        r_all+=r\n",
    "    Ma_p=p_all/count\n",
    "    Ma_r=r_all/count\n",
    "    aspect_Macro_F1 = 2*Ma_p*Ma_r/(Ma_p+Ma_r)\n",
    "\n",
    "    return aspect_Macro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(tqdm(examples)):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(\n",
    "                        input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15008/15008 [00:07<00:00, 1984.44it/s]\n",
      "100%|██████████| 3748/3748 [00:01<00:00, 2013.66it/s]\n",
      "100%|██████████| 7516/7516 [00:04<00:00, 1839.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15008, 3748, 7516)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "processor = Sentihood_NLI_M_Processor()\n",
    "dta_dir = './data/sentihood/bert-pair/'\n",
    "label_list = processor.get_labels()\n",
    "train_examples=processor.get_train_examples(dta_dir)\n",
    "test_examples=processor.get_test_examples(dta_dir)\n",
    "dev_examples=processor.get_dev_examples(dta_dir)\n",
    "train_features = convert_examples_to_features(train_examples, label_list, 128, tokenizer)\n",
    "dev_features = convert_examples_to_features(dev_examples, label_list, 128, tokenizer)\n",
    "test_features = convert_examples_to_features(test_examples, label_list, 128, tokenizer)\n",
    "len(train_features), len(dev_features), len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.label_id = label_id\n",
    "# ['input_word_ids', 'input_mask', 'input_type_ids']\n",
    "train_x = {} \n",
    "train_x['input_word_ids']=np.array([np.array(train_features[i].input_ids) for i in range(len(train_features))])\n",
    "train_x['input_mask']=np.array([np.array(train_features[i].input_mask) for i in range(len(train_features))])\n",
    "train_x['input_type_ids']=np.array([np.array(train_features[i].segment_ids) for i in range(len(train_features))])\n",
    "\n",
    "test_x = {} \n",
    "test_x['input_word_ids']=np.array([np.array(test_features[i].input_ids) for i in range(len(test_features))])\n",
    "test_x['input_mask']=np.array([np.array(test_features[i].input_mask) for i in range(len(test_features))])\n",
    "test_x['input_type_ids']=np.array([np.array(test_features[i].segment_ids) for i in range(len(test_features))])\n",
    "\n",
    "dev_x = {} \n",
    "dev_x['input_word_ids']=np.array([np.array(dev_features[i].input_ids) for i in range(len(dev_features))])\n",
    "dev_x['input_mask']=np.array([np.array(dev_features[i].input_mask) for i in range(len(dev_features))])\n",
    "dev_x['input_type_ids']=np.array([np.array(dev_features[i].segment_ids) for i in range(len(dev_features))])\n",
    "\n",
    "train_y = np.array(to_categorical([train_features[i].label_id for i in range(len(train_features))]))\n",
    "dev_y = np.array(to_categorical([dev_features[i].label_id for i in range(len(dev_features))]))\n",
    "test_y = np.array(to_categorical([test_features[i].label_id for i in range(len(test_features))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our method on the SentiHood dataset, which consists of 5,215 sentences, 3,862 of which contain a single target,   \n",
    "and the remainder multiple targets. Each sentence contains a list of target-aspect pairs {t, a} with the  \n",
    "sentiment polarity y. Ultimately, given a sentences and the target t in the sentence, we need to:  \n",
    "(1) detect the mention of an aspect a for the target t;  \n",
    "(2) determine the positive or negative sentiment polarity y for detected target-aspect pairs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence base, multi-task approach\n",
    "def create_classify_model(data_size, batch_size = 16, epochs=10, category_len = 8, sentiment_len = 2):\n",
    "    import model.optimization as optimization\n",
    "    input1 = Input(shape=(128,), name='input_word_ids', dtype=tf.int32)\n",
    "    input2 = Input(shape=(128,),name='input_mask', dtype=tf.int32)\n",
    "    input3 = Input(shape=(128,),name='input_type_ids', dtype=tf.int32)\n",
    "    bert_layer = hub.KerasLayer(BERT_src, trainable=True, output_key='pooled_output', name='bert_layer')\n",
    "    output = bert_layer({'input_word_ids':input1, 'input_mask':input2, 'input_type_ids':input3})\n",
    "#     output = Dense(128, name = 'presentation_')(output)\n",
    "    \n",
    "    sentiment_output = Dense(64, activation='relu', name = 'sentiment_pre', \n",
    "                             kernel_initializer=keras.initializers.glorot_normal(0), bias_initializer='zeros')(output)\n",
    "#     sentiment_output = Dropout(0.2, name='sentiment_drop')(sentiment_output)\n",
    "    sentiment_output = Dense(sentiment_len, activation='softmax', name = 'sentiment', \n",
    "                             kernel_initializer=keras.initializers.glorot_normal(0), bias_initializer='zeros')(sentiment_output) #softmax會讓所有的output總和=1\n",
    "    \n",
    "    output_model = Model(inputs = [input1, input2, input3], outputs = sentiment_output)\n",
    "    optimizer = optimization.create_optimizer(\n",
    "    5e-5, (data_size//batch_size)*epochs, int((epochs*data_size*0.1)//batch_size), 0.0, 'adamw')\n",
    "    \n",
    "    output_model.compile(optimizer=optimizer, \n",
    "                         loss={'sentiment':'categorical_crossentropy'})#'categorical_crossentropy'})\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence base, only sentence approach\n",
    "#Gradient Reverse Layer\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradReverse(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)\n",
    "    \n",
    "def create_temp_model(data_size, batch_size = 16, epochs=10, domain_size = 1, sentiment_len = 2):\n",
    "    import model.optimization as optimization\n",
    "    input1 = Input(shape=(128,), name='input_word_ids', dtype=tf.int32)\n",
    "    input2 = Input(shape=(128,),name='input_mask', dtype=tf.int32)\n",
    "    input3 = Input(shape=(128,),name='input_type_ids', dtype=tf.int32)\n",
    "    bert_layer = hub.KerasLayer(BERT_src, trainable=True, output_key='pooled_output', name='bert_layer')\n",
    "    output = bert_layer({'input_word_ids':input1, 'input_mask':input2, 'input_type_ids':input3})\n",
    "#     output = Dense(128, name = 'presentation_')(output)\n",
    "    \n",
    "    sentiment_output = Dense(64, activation='relu', name = 'sentiment_pre', \n",
    "                             kernel_initializer=keras.initializers.he_normal(0), bias_initializer='zeros')(output)\n",
    "#     sentiment_output = Dropout(0.2, name='sentiment_drop')(sentiment_output)\n",
    "    sentiment_output = Dense(sentiment_len, activation='softmax', name = 'sentiment', \n",
    "                             kernel_initializer=keras.initializers.he_normal(0), bias_initializer='zeros')(sentiment_output) #softmax會讓所有的output總和=1\n",
    "    if domain_size>1:\n",
    "        temp_output = GradReverse()(output)\n",
    "        dis_output = Dense(64, activation='relu', name = 'dis_pre', \n",
    "                             kernel_initializer=keras.initializers.he_normal(0), bias_initializer='zeros')(temp_output)\n",
    "        dis_output = Dense(domain_size, activation='softmax', name = 'discriminator', \n",
    "                             kernel_initializer=keras.initializers.he_normal(0), bias_initializer='zeros')(dis_output) #softmax會讓所有的output總和=1\n",
    "\n",
    "    optimizer = optimization.create_optimizer(\n",
    "    5e-5, (data_size//batch_size)*epochs, int((epochs*data_size*0.1)//batch_size), 0.0, 'adamw')\n",
    "    if domain_size>1:        \n",
    "        output_model = Model(inputs = [input1, input2, input3], outputs = [sentiment_output, dis_output])\n",
    "        output_model.compile(optimizer=optimizer, \n",
    "                             loss={'sentiment':'categorical_crossentropy', 'discriminator':'categorical_crossentropy'},\n",
    "                            loss_weights={'sentiment':1., 'discriminator':.3})\n",
    "        only_sentiment_model = Model(inputs = [input1, input2, input3], outputs = sentiment_output)\n",
    "    else:\n",
    "        output_model = Model(inputs = [input1, input2, input3], outputs = sentiment_output)\n",
    "        output_model.compile(optimizer=optimizer, \n",
    "                             loss={'sentiment':'categorical_crossentropy'})\n",
    "        \n",
    "    return output_model, only_sentiment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import model.optimization as optimization\n",
    "try:\n",
    "    del tmp_model\n",
    "except:\n",
    "    ;\n",
    "data_size=1600\n",
    "batch_size=32\n",
    "epochs=5\n",
    "optimizer = optimization.create_optimizer(5e-5, (data_size//batch_size)*epochs, int((epochs*data_size*0.1)//batch_size), 0.0, 'adamw')\n",
    "from tensorflow.keras.models import load_model\n",
    "print('loading model...')\n",
    "tmp_model = load_model('./Meta-ACS_weight_save/eng_rep_adv.h5', custom_objects={'KerasLayer':BERT_LAYER, 'AdamWeightDecay':optimizer})\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: 1\n",
      "train size: 12006\n",
      "Epoch 1/20\n",
      "376/376 [==============================] - 202s 504ms/step - loss: 0.7622 - val_loss: 0.3167\n",
      "Epoch 2/20\n",
      "376/376 [==============================] - 200s 531ms/step - loss: 0.3127 - val_loss: 0.2227\n",
      "Epoch 3/20\n",
      "376/376 [==============================] - 202s 539ms/step - loss: 0.1896 - val_loss: 0.2307\n",
      "Epoch 4/20\n",
      "376/376 [==============================] - 204s 541ms/step - loss: 0.1358 - val_loss: 0.2335\n",
      "aspect acc.: 0.7264502394890899\n",
      "aspect macro-f1: 0.8136581017590614\n",
      "sentiment acc.: 0.8930921052631579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          正向       0.89      0.96      0.92       810\n",
      "          負向       0.91      0.76      0.83       406\n",
      "\n",
      "    accuracy                           0.89      1216\n",
      "   macro avg       0.90      0.86      0.87      1216\n",
      "weighted avg       0.89      0.89      0.89      1216\n",
      "\n",
      "meta: 1\n",
      "train size: 15008\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 259s 529ms/step - loss: 0.7290 - val_loss: 0.2972\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 249s 532ms/step - loss: 0.2671 - val_loss: 0.2272\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 249s 531ms/step - loss: 0.1658 - val_loss: 0.1786\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 249s 532ms/step - loss: 0.1387 - val_loss: 0.2437\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 249s 532ms/step - loss: 0.1012 - val_loss: 0.2603\n",
      "aspect acc.: 0.7626397019691326\n",
      "aspect macro-f1: 0.8302260527938087\n",
      "sentiment acc.: 0.928453947368421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          正向       0.95      0.94      0.95       810\n",
      "          負向       0.88      0.91      0.89       406\n",
      "\n",
      "    accuracy                           0.93      1216\n",
      "   macro avg       0.92      0.92      0.92      1216\n",
      "weighted avg       0.93      0.93      0.93      1216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train n 個 y/n模型\n",
    "# data_ratios = [.01, .05, .1, .2, .3, .4, .5, .8, 1]\n",
    "data_ratios = [.8, 1]\n",
    "epochs = 20\n",
    "for meta in range(2):\n",
    "    if meta==0: continue;\n",
    "    for data_ratio in data_ratios:\n",
    "        datasize = int(data_ratio*len(train_y))\n",
    "        x, y = sample_data([train_x, train_y], datasize, random_=False)\n",
    "        print('meta:', meta)\n",
    "        print('train size:', datasize)\n",
    "        try:R\n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "        model = create_classify_model(data_size=len(y), epochs=epochs, sentiment_len = 3)\n",
    "        if meta:\n",
    "            update_weights_forsame(model, tmp_model)\n",
    "        model.fit(x, y, epochs=epochs, verbose=1, batch_size=32, validation_data=(dev_x, dev_y), callbacks=[callback])\n",
    "        sent_pred = model.predict(test_x)\n",
    "        del model\n",
    "        #evaluate\n",
    "        from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "        idx2sent = ['無', '正向', '負向']\n",
    "        sent_predict = [idx2sent[np.argmax(i)] for i in sent_pred]\n",
    "        sent_ans = [idx2sent[np.argmax(i)] for i in test_y]\n",
    "\n",
    "\n",
    "        total_cases = len(sent_predict)/4\n",
    "        true_cases = 0\n",
    "        for i in range(int(total_cases)):\n",
    "            if sent_predict[i*4]!=sent_ans[i*4]: continue\n",
    "            if sent_predict[i*4+1]!=sent_ans[i*4+1]: continue\n",
    "            if sent_predict[i*4+2]!=sent_ans[i*4+2]: continue\n",
    "            if sent_predict[i*4+3]!=sent_ans[i*4+3]: continue\n",
    "            true_cases+=1\n",
    "        print('aspect acc.:',true_cases/total_cases)\n",
    "        print('aspect macro-f1:', sentihood_macro_F1(sent_ans, sent_predict))\n",
    "\n",
    "        sent_predict = [idx2sent[np.argmax(i)] for i in sent_pred]\n",
    "        sent_ans = [idx2sent[np.argmax(i)] for i in test_y]\n",
    "        sent_predict, sent_answer = [], []\n",
    "        for i in range(len(sent_ans)):\n",
    "            if sent_ans[i]!='無':\n",
    "                sent_answer.append(sent_ans[i])\n",
    "                if sent_pred[i][1]>=sent_pred[i][2]:\n",
    "                    sent_predict.append('正向')\n",
    "                else:\n",
    "                    sent_predict.append('負向')\n",
    "        print('sentiment acc.:', accuracy_score(sent_answer, sent_predict))  \n",
    "        print(classification_report(sent_answer, sent_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class src_Processor(DataProcessor):\n",
    "    \"\"\"Processor for the Sentihood data set.\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        data_dir+='train.json'\n",
    "        train_data = parse_src_json(data_dir)\n",
    "        return self._create_examples(train_data, \"train\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        data_dir+='test.json'\n",
    "        test_data = parse_src_json(data_dir)\n",
    "        return self._create_examples(test_data, \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return ['Neutral', 'Positive', 'Negative']\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        labels = {'中立':'Neutral', '正向':'Positive', '負向':'Negative'}\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(str(line['sentence']))\n",
    "            text_b = tokenization.convert_to_unicode(str(line['target']))\n",
    "            label = tokenization.convert_to_unicode(str(labels[line['sentiment']]))\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples        \n",
    "\n",
    "def parse_src_json(in_file):\n",
    "    with open(in_file, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "    ret = []\n",
    "    for d in data:\n",
    "        ret.append(json.loads(d))\n",
    "    return ret        \n",
    "in_file = './data/ABSA_Eng/laptop_' \n",
    "processor = src_Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 183/2328 [00:00<00:01, 1829.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:2328, test:638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2328/2328 [00:01<00:00, 1770.93it/s]\n",
      "100%|██████████| 638/638 [00:00<00:00, 2100.40it/s]\n",
      "  6%|▌         | 214/3608 [00:00<00:01, 2127.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:3608, test:1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3608/3608 [00:01<00:00, 2025.84it/s]\n",
      "100%|██████████| 1120/1120 [00:00<00:00, 1765.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_x_y(in_file, processor):\n",
    "    train_data = processor.get_train_examples(in_file)\n",
    "    test_data = processor.get_test_examples(in_file)\n",
    "    label_list = processor.get_labels()\n",
    "    print('train:{}, test:{}'.format(len(train_data), len(test_data)))\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    train_features = convert_examples_to_features(train_data, label_list, 128, tokenizer)\n",
    "    test_features = convert_examples_to_features(test_data, label_list, 128, tokenizer)\n",
    "\n",
    "    train_x = {} \n",
    "    train_x['input_word_ids']=np.array([np.array(train_features[i].input_ids) for i in range(len(train_features))])\n",
    "    train_x['input_mask']=np.array([np.array(train_features[i].input_mask) for i in range(len(train_features))])\n",
    "    train_x['input_type_ids']=np.array([np.array(train_features[i].segment_ids) for i in range(len(train_features))])\n",
    "\n",
    "    test_x = {} \n",
    "    test_x['input_word_ids']=np.array([np.array(test_features[i].input_ids) for i in range(len(test_features))])\n",
    "    test_x['input_mask']=np.array([np.array(test_features[i].input_mask) for i in range(len(test_features))])\n",
    "    test_x['input_type_ids']=np.array([np.array(test_features[i].segment_ids) for i in range(len(test_features))])\n",
    "\n",
    "    train_y = np.array(to_categorical([train_features[i].label_id for i in range(len(train_features))]))\n",
    "    test_y = np.array(to_categorical([test_features[i].label_id for i in range(len(test_features))]))\n",
    "    \n",
    "    train = {'x':train_x, 'y':train_y}\n",
    "    test = {'x':test_x, 'y':test_y}\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "processor = src_Processor()\n",
    "laptop_train, laptop_test = get_x_y('./data/ABSA_Eng/laptop_', processor )\n",
    "restaurant_train, restaurant_test = get_x_y('./data/ABSA_Eng/restaurant_', processor )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adv_reptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_data(data_list, datasize, random_=True):\n",
    "    #data_list = [BERT_x, sentiment]\n",
    "    if random_:\n",
    "        samples = random.sample(range(len(data_list[1])), datasize)\n",
    "    else:\n",
    "        samples = list(range(datasize))        \n",
    "    bert_x = data_list[0]\n",
    "    bert_x = {k:np.array([bert_x[k][i] for i in samples]) for k in bert_x.keys()}\n",
    "    sentiment = np.array(data_list[1])\n",
    "    sentiment = np.array([sentiment[i] for i in samples])\n",
    "    return bert_x, sentiment\n",
    "def model_get_weight(model, keyword='', not_=False):\n",
    "    origin_weight = []\n",
    "    for layer in model.layers:\n",
    "        if not_:\n",
    "            if not layer.name.startswith(keyword): \n",
    "                origin_weight.append(np.array(layer.get_weights()))\n",
    "        else:\n",
    "            if layer.name.startswith(keyword): \n",
    "                origin_weight.append(np.array(layer.get_weights()))\n",
    "    return np.array(origin_weight)\n",
    "\n",
    "def update_weights(model, update_weight, keyword='', not_=False):\n",
    "    k=0\n",
    "    for layer in model.layers:\n",
    "        if not_:\n",
    "            if not layer.name.startswith(keyword):\n",
    "                layer.set_weights(update_weight[k])\n",
    "                k+=1\n",
    "        else:\n",
    "            if layer.name.startswith(keyword):\n",
    "                layer.set_weights(update_weight[k])\n",
    "                k+=1\n",
    "def update_weights_forsame(model, model_src):\n",
    "    for layer in model.layers:\n",
    "        flag = False\n",
    "        for layer_src in model_src.layers:\n",
    "            if layer.name==layer_src.name and len(layer.get_weights())==len(layer_src.get_weights()) and flag==False:\n",
    "                try: \n",
    "                    layer.set_weights(layer_src.get_weights())\n",
    "                    flag = True\n",
    "                except:\n",
    "                    print('error!')\n",
    "        if flag==False:\n",
    "            print('model layer: \"', layer.name, '\" not in source model')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to train!\n",
      "itr = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.1 \tloss [1.04505, 0.69711] \t adv loss 1.14007 \t spend 44\n",
      "lr 0.09667 \tloss [0.54643, 0.60665] \t adv loss 1.8317 \t spend 64\n",
      "lr 0.09333 \tloss [0.19355, 0.28034] \t adv loss 3.52664 \t spend 84\n",
      "lr 0.09 \tloss [0.28936, 0.38339] \t adv loss 8.42871 \t spend 104\n",
      "lr 0.08667 \tloss [0.22859, 0.43503] \t adv loss 8.10102 \t spend 125\n",
      "lr 0.08333 \tloss [0.34958, 0.41372] \t adv loss 10.74382 \t spend 146\n",
      "lr 0.08 \tloss [0.4784, 0.53215] \t adv loss 12.07064 \t spend 167\n",
      "lr 0.07667 \tloss [0.42843, 0.42905] \t adv loss 12.06102 \t spend 189\n",
      "lr 0.07333 \tloss [0.52688, 0.41826] \t adv loss 13.21854 \t spend 210\n",
      "lr 0.07 \tloss [0.4524, 0.52519] \t adv loss 13.71357 \t spend 232\n",
      "itr = 10\n",
      "lr 0.06667 \tloss [0.55659, 0.38213] \t adv loss 14.01343 \t spend 254\n",
      "lr 0.06333 \tloss [0.56347, 0.35216] \t adv loss 14.31878 \t spend 276\n",
      "lr 0.06 \tloss [0.5411, 0.56021] \t adv loss 14.09672 \t spend 298\n",
      "lr 0.05667 \tloss [0.60183, 0.42578] \t adv loss 13.82195 \t spend 321\n",
      "lr 0.05333 \tloss [0.63901, 0.38216] \t adv loss 13.99214 \t spend 343\n",
      "lr 0.05 \tloss [0.57934, 0.41213] \t adv loss 14.23388 \t spend 366\n",
      "lr 0.04667 \tloss [0.61731, 0.45081] \t adv loss 13.94878 \t spend 389\n",
      "lr 0.04333 \tloss [0.59681, 0.46539] \t adv loss 14.21694 \t spend 411\n",
      "lr 0.04 \tloss [0.605, 0.36477] \t adv loss 14.23662 \t spend 434\n",
      "lr 0.03667 \tloss [0.56243, 0.43208] \t adv loss 14.03206 \t spend 456\n",
      "itr = 20\n",
      "lr 0.03333 \tloss [0.62514, 0.4251] \t adv loss 13.55708 \t spend 479\n",
      "lr 0.03 \tloss [0.61781, 0.5175] \t adv loss 13.32732 \t spend 502\n",
      "lr 0.02667 \tloss [0.63712, 0.42774] \t adv loss 13.36999 \t spend 525\n",
      "lr 0.02333 \tloss [0.66197, 0.57258] \t adv loss 13.23774 \t spend 547\n",
      "lr 0.02 \tloss [0.53241, 0.51874] \t adv loss 13.14034 \t spend 570\n",
      "lr 0.01667 \tloss [0.65496, 0.58372] \t adv loss 13.11063 \t spend 593\n",
      "lr 0.01333 \tloss [0.7371, 0.55348] \t adv loss 12.7714 \t spend 616\n",
      "lr 0.01 \tloss [0.73526, 0.6018] \t adv loss 12.36496 \t spend 639\n",
      "lr 0.00667 \tloss [0.78049, 0.60911] \t adv loss 12.24324 \t spend 662\n",
      "lr 0.00333 \tloss [0.80836, 0.57531] \t adv loss 11.9749 \t spend 685\n",
      "total spend 685 seconds\n"
     ]
    }
   ],
   "source": [
    "outer_iteration = 30 #一個iteration大概3.1秒(一個domain)\n",
    "inner_iteration = 20\n",
    "epochs = inner_iteration\n",
    "datasize_per_task = 32\n",
    "meta_step_size = 0.1\n",
    "domain_adver = 2\n",
    "import time\n",
    "starttime = time.time()\n",
    "src_x = [laptop_train['x'], restaurant_train['x']]\n",
    "src_y = [laptop_train['y'], restaurant_train['y']]\n",
    "tmp_model, save_model = create_temp_model(data_size=len(laptop_train['y']), epochs=epochs, batch_size=datasize_per_task, domain_size = domain_adver, sentiment_len = 3)\n",
    "\n",
    "print('start to train!')\n",
    "for itr in range(outer_iteration):\n",
    "    if itr%10==0:\n",
    "        print('itr =',itr)\n",
    "    done_step = itr/outer_iteration\n",
    "    cur_meta_step_size = (1-done_step)*meta_step_size\n",
    "    origin_weights = model_get_weight(tmp_model)\n",
    "    \n",
    "    new_weights = []\n",
    "    losses = []\n",
    "    advloss = []\n",
    "    adv_loss = 0\n",
    "    domain_num = 0\n",
    "    \n",
    "    for x, y in zip(src_x, src_y):     \n",
    "        tmp_train_x, tmp_sentiment = sample_data([x, y], datasize_per_task)  \n",
    "        loss = []\n",
    "        for i in range(inner_iteration):\n",
    "            if domain_adver>1:\n",
    "                domain_ans = to_categorical(len(tmp_sentiment)*[domain_num], num_classes=domain_adver)\n",
    "                total_loss = tmp_model.train_on_batch(x=tmp_train_x, y=[tmp_sentiment, domain_ans])                \n",
    "                loss = round(total_loss[1], 5) \n",
    "                adv_loss = round(total_loss[2], 5)\n",
    "            else:\n",
    "                loss = tmp_model.train_on_batch(x=tmp_train_x, y=tmp_sentiment)\n",
    "                loss = round(loss, 5)\n",
    "                \n",
    "        new_weights.append(model_get_weight(tmp_model))\n",
    "        update_weights(tmp_model, origin_weights)\n",
    "        losses.append(loss)\n",
    "        advloss.append(adv_loss) #只看最後一個的loss好了\n",
    "        domain_num+=1\n",
    "    #update weights\n",
    "    new_weights = np.array(new_weights)\n",
    "    new_weight = new_weights[0]\n",
    "    for i in range(len(new_weights)-1):\n",
    "        new_weight+=new_weights[i+1]\n",
    "    new_weight/=len(new_weights)    \n",
    "    new_weight = origin_weights + ((new_weight-origin_weights)*cur_meta_step_size)\n",
    "    print('lr', round(cur_meta_step_size, 5), '\\tloss', losses, '\\t adv loss', round(np.mean(advloss), 5), '\\t spend', int(time.time()-starttime))\n",
    "    update_weights(tmp_model, new_weight)\n",
    "    del new_weight, origin_weights, new_weights\n",
    "print('total spend {} seconds'.format(int(time.time()-starttime)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
